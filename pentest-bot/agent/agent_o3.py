# pentest_multi_turn_agent.py
"""A self‚Äëcontained multi‚Äëturn pentesting agent runner.

This expands the single‚Äëturn `PentestBot` prototype into a loop that
iteratively generates Python exploit scripts, executes them, feeds the
results back to the LLM, and stops when an exploit is confirmed or a
configured step‚Äëlimit is reached.

Dependencies
------------
* johnllm‚Äëprovided `LMP` and `LLMModel` abstractions
* pydantic >= 2.0

The code purposefully keeps the runtime sandbox simple (built on `exec`).
For production use you **must** swap in a hardened sandbox (e.g. "python‚Äëseccomp",
`subprocess` inside a Docker / Firecracker micro‚ÄëVM, etc.).
"""
from __future__ import annotations

import traceback
import json
from contextlib import redirect_stderr, redirect_stdout
from io import StringIO
from types import SimpleNamespace
from typing import Dict, Optional, Tuple

from johnllm import LMP, LLMModel  # type: ignore ‚Äì¬†project‚Äëspecific
from pydantic import BaseModel
from logging import getLogger

from .logger import setup_agent_logger

logger = getLogger("agentlog")

PORTSWIGGER_LABS = "scripts/portswigger/port_swigger_labs.json"

class AgentOutput(BaseModel):
    """LLM response schema.

    The model must return JSON with these two keys. `reflection` is the
    natural‚Äëlanguage reasoning; `script` is the Python code to execute next.
    """
    reflection: str
    script: str

    def is_complete(self):
        return "TASK_COMPLETE" in self.script or "TASK_COMPLETE" in self.reflection


class PentestBot(LMP):
    """Language‚Äëmodel program that drives each reasoning step."""

    prompt = (
        """
{% if problem_stmt %}
<problem_stmt>
Here is the CTF problem statement:
{{ problem_stmt }}
</problem_stmt>

{% elif prev_turn %}
<prev_reflection>
Here is the reflection from the previous action
{{ prev_turn.reflection }}
</prev_reflection>

<prev_script>
Here is the generated script:
{{ prev_turn.script }}
</prev_script>

<prev_output>
Here is the output from the previous action:
{{ prev_turn.output }}
{% endif %}
</prev_output>

You are a professional web-app CTF/pentesting agent named **PentestBot**.
Your goal is to discover and exploit a vulnerability in the target.
The *only* interaction channel with the target is a Python script that will be
executed immediately. You will receive the combined stdout/stderr of that
execution to inform your next step.

{% if success_condition %}
<success_condition>
If the following condition 
is observed in the script output, then return the keyword TASK_COMPLETE in your output:
{{ success_condition}}
<success_condition>
{% endif %}

Recommended workflow per turn:
1. Analyse the previous execution script and output.
2. Decide the next tactical objective.
3. Emit a concise reflection explaining your reasoning.
4. Emit a *complete* Python script implementing that objective.
"""
    )
    response_format = AgentOutput


###############################################################################
# Sandbox interpreter
###############################################################################


class PythonInterpreter:
    """Very small wrapper that runs untrusted Python code and captures output.

    For demonstration purposes `exec` is used. In real life, replace this with
    a hardened sandbox (Docker, gVisor, Firecracker, etc.) and apply resource
    limits.
    """

    def __init__(self, shared_globals: Optional[Dict] = None) -> None:
        # Allow stateful payloads (e.g. re‚Äëusing imported `requests` sessions)
        self._globals: Dict = shared_globals or {}

    # ---------------------------------------------------------------------
    # Public helpers
    # ---------------------------------------------------------------------

    def run(self, code: str) -> str:
        """Execute *code* and return the concatenated stdout+stderr text."""

        stdout_buf = StringIO()
        stderr_buf = StringIO()

        try:
            with redirect_stdout(stdout_buf), redirect_stderr(stderr_buf):
                exec(code, self._globals, {})
        except Exception:  # pylint: disable=broad-except
            traceback.print_exc(file=stderr_buf)

        stdout_text = stdout_buf.getvalue()
        stderr_text = stderr_buf.getvalue()
        return stdout_text + ("\n" + stderr_text if stderr_text else "")


###############################################################################
# Multi‚Äëturn session driver
###############################################################################


class PentestSession:
    """Drives the LLM ‚Üî interpreter feedback loop."""

    def __init__(
        self,
        problem_stmt: str,
        model: LLMModel,
        model_name: str = "gpt-4.1",
        max_steps: int = 12,
        success_condition: str = "",
    ) -> None:
        self.problem_stmt = problem_stmt
        self.model_name = model_name
        self.max_steps = max_steps
        self.success_condition = success_condition
        self.logger = getLogger("agentlog")

        self._model = model
        self._agent = PentestBot()
        self._interp = PythonInterpreter()
        self._success = False
        self._steps = 0

        self.logger.info("IM RUNNING!!!")

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def run(self) -> None:
        """Run the conversation loop until success or exhaustion."""

        prev_turn: Optional[SimpleNamespace] = None

        for step in range(1, self.max_steps + 1):
            self.logger.info("========== STEP %d ==========" , step)

            # Build prompt args for this turn
            prompt_args = {
                "problem_stmt": self.problem_stmt if step == 1 else None,
                "prev_turn": prev_turn,
                "success_condition": self.success_condition,
            }
            self.logger.info("Prompt: \n%s", self._agent._prepare_prompt(templates={}, **prompt_args))

            # Call the LLM
            agent_out: AgentOutput = self._agent.invoke(
                model=self._model,
                model_name=self.model_name,
                prompt_args=prompt_args,
            )
            if agent_out.is_complete():
                self.logger.info("Agent successfully completed task.")
                self._success = True
                return

            # Log reflection & script
            self.logger.info("Reflection:\n%s", agent_out.reflection.strip())
            self.logger.info("Generated script:\n%s", agent_out.script.rstrip())

            # Execute the script
            execution_result = self._interp.run(agent_out.script)
            self.logger.info("Execution output (truncated to 4k chars):\n%s", execution_result[:4096])

            # Prepare previous‚Äëturn context for next iteration
            prev_turn = SimpleNamespace(
                reflection=agent_out.reflection,
                script=agent_out.script,
                output=execution_result,
            )
            self._steps += 1

        self.logger.warning("üíÄ Maximum steps (%d) reached without confirmed exploit.", self.max_steps)

    def result(self):
        return self._success, self._steps, self.max_steps

def start_agent(lab_info, eval_name, model, max_steps: int = 8) -> tuple[bool, int, int]:  # pragma: no cover (simple demo runner)
    labs = json.loads(open(PORTSWIGGER_LABS, "r").read())
    url, vuln, lab_ind = lab_info
    problem_statement = labs[vuln][lab_ind]["prompt"].format(url=url)
    success_condition = labs[vuln][lab_ind]["success_condition"].format(url=url)

    setup_agent_logger(eval_name)
    logger.info(f"Lab info: {url}, {vuln}, {lab_ind}: \n{problem_statement}")

    session = PentestSession(
        problem_stmt=problem_statement,
        model=model,
        # TODO: this success condition should go into PORTSWIGGER_JSON
        success_condition=success_condition, 
        max_steps=max_steps)
    session.run()
    return session.result()

if __name__ == "__main__":    
    from concurrent.futures import ThreadPoolExecutor
    from dataclasses import dataclass
    from typing import Dict

    @dataclass 
    class AgentResult:
        eval_name: str
        success: bool
        steps: int
        max_steps: int

    MAX_STEPS = 12
    LAB_URLS = [
        ("https://0a11004504acfab081628521007d003e.web-security-academy.net/", "sql_injection", 5),
        ("https://0af9006504abccc680590da200700062.web-security-academy.net/", "sql_injection", 4),
        ("https://0a77004c03d2987f8311106800ce00d7.web-security-academy.net/", "sql_injection", 8),
        ("https://0af100db035b335182358b9000d300eb.web-security-academy.net/", "sql_injection", 6),
        ("https://0ae7001f0471345881c4c06400a900bd.web-security-academy.net/", "sql_injection", 9),
        ("https://0a220028040f19f7805a0dd200a2008f.web-security-academy.net/", "sql_injection", 3),
        ("https://0a4800360307de0980d803a5007700a8.web-security-academy.net/", "sql_injection", 2),
    ]

    results: Dict[str, AgentResult] = {}
    model = LLMModel()
    
    def run_agent(lab_info: Tuple[str, str, int], i: int) -> None:
        eval_name = f"{lab_info[1]}_{lab_info[2]}"
        success, steps, max_steps = start_agent(lab_info, eval_name, model, max_steps=MAX_STEPS)
        results[eval_name] = AgentResult(eval_name, success, steps, max_steps)

    # Run agents in parallel
    with ThreadPoolExecutor() as executor:
        executor.map(run_agent, LAB_URLS, range(len(LAB_URLS)))

    # Print results
    print("\nAgent Results:")
    print("-" * 50)
    for eval_name, result in results.items():
        status = "‚úÖ Success" if result.success else "‚ùå Failed"
        print(f"{eval_name}: {status} (Steps: {result.steps}/{result.max_steps})")

    print("MODEL COST: ", model.cost)