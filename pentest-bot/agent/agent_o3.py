# pentest_multi_turn_agent.py
"""A selfâ€‘contained multiâ€‘turn pentesting agent runner.

This expands the singleâ€‘turn `PentestBot` prototype into a loop that
iteratively generates Python exploit scripts, executes them, feeds the
results back to the LLM, and stops when an exploit is confirmed or a
configured stepâ€‘limit is reached.

Dependencies
------------
* johnllmâ€‘provided `LMP` and `LLMModel` abstractions
* pydantic >= 2.0

The code purposefully keeps the runtime sandbox simple (built on `exec`).
For production use you **must** swap in a hardened sandbox (e.g. "pythonâ€‘seccomp",
`subprocess` inside a Docker / Firecracker microâ€‘VM, etc.).
"""
from __future__ import annotations

import logging
import sys
import json
import traceback
from contextlib import redirect_stderr, redirect_stdout
from io import StringIO
from types import SimpleNamespace
from typing import Dict, Optional

from johnllm import LMP, LLMModel  # type: ignore â€“Â projectâ€‘specific
from pydantic import BaseModel

from scripts.portswigger.start_lab import start_lab, PORTSWIGGER_JSON
from logging import getLogger

logger = getLogger("agentlog")

###############################################################################
# Agent prompt & singleâ€‘turn schema
###############################################################################


class AgentOutput(BaseModel):
    """LLM response schema.

    The model must return JSON with these two keys. `reflection` is the
    naturalâ€‘language reasoning; `script` is the Python code to execute next.
    """
    reflection: str
    script: str

    def is_complete(self):
        return "TASK_COMPLETE" in self.script or "TASK_COMPLETE" in self.reflection


class PentestBot(LMP):
    """Languageâ€‘model program that drives each reasoning step."""

    prompt = (
        """
{% if problem_stmt %}
Here is the CTF problem statement:
{{ problem_stmt }}

{% elif prev_turn %}
Here is the reflection from the previous action
{{ prev_turn.reflection }}

Here is the generated script:
{{ prev_turn.script }}

Here is the output from the previous action:
{{ prev_turn.output }}
{% endif %}

You are a professional web-app CTF/pentesting agent named **PentestBot**.
Your goal is to discover and exploit a vulnerability in the target.
The *only* interaction channel with the target is a Python script that will be
executed immediately. You will receive the combined stdout/stderr of that
execution to inform your next step.

Your reply **must** be valid JSON with exactly the keys `reflection` and
`script` (matching the schema supplied). The `script` **must** be self-contained
Python.

{% if success_condition %}
If the following condition is observed in the script output, then return the keyword TASK_COMPLETE in your output:
{{ success_condition}}
{% endif %}

Recommended workflow per turn:
1. Analyse the latest execution output.
2. Decide the next tactical objective.
3. Emit a concise reflection explaining your reasoning.
4. Emit a *complete* Python script implementing that objective.
"""
    )
    response_format = AgentOutput


###############################################################################
# Sandbox interpreter
###############################################################################


class PythonInterpreter:
    """Very small wrapper that runs untrusted Python code and captures output.

    For demonstration purposes `exec` is used. In real life, replace this with
    a hardened sandbox (Docker, gVisor, Firecracker, etc.) and apply resource
    limits.
    """

    def __init__(self, shared_globals: Optional[Dict] = None) -> None:
        # Allow stateful payloads (e.g. reâ€‘using imported `requests` sessions)
        self._globals: Dict = shared_globals or {}

    # ---------------------------------------------------------------------
    # Public helpers
    # ---------------------------------------------------------------------

    def run(self, code: str) -> str:
        """Execute *code* and return the concatenated stdout+stderr text."""

        stdout_buf = StringIO()
        stderr_buf = StringIO()

        try:
            with redirect_stdout(stdout_buf), redirect_stderr(stderr_buf):
                exec(code, self._globals, {})
        except Exception:  # pylint: disable=broad-except
            traceback.print_exc(file=stderr_buf)

        stdout_text = stdout_buf.getvalue()
        stderr_text = stderr_buf.getvalue()
        return stdout_text + ("\n" + stderr_text if stderr_text else "")


###############################################################################
# Multiâ€‘turn session driver
###############################################################################


class PentestSession:
    """Drives the LLM â†” interpreter feedback loop."""

    def __init__(
        self,
        problem_stmt: str,
        model_name: str = "gpt-4.1",
        max_steps: int = 12,
        success_condition: str = "",
    ) -> None:
        self.problem_stmt = problem_stmt
        self.model_name = model_name
        self.max_steps = max_steps
        self.success_condition = success_condition

        self._model = LLMModel()
        self._agent = PentestBot()
        self._interp = PythonInterpreter()

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def run(self) -> None:
        """Run the conversation loop until success or exhaustion."""

        prev_turn: Optional[SimpleNamespace] = None

        for step in range(1, self.max_steps + 1):
            logger.info("========== STEP %d ==========" , step)

            # Build prompt args for this turn
            prompt_args = {
                "problem_stmt": self.problem_stmt if step == 1 else None,
                "prev_turn": prev_turn,
                "success_condition": self.success_condition,
            }
            logger.info("Prompt: \n%s", self._agent._prepare_prompt(templates={}, **prompt_args))

            # Call the LLM
            agent_out: AgentOutput = self._agent.invoke(
                model=self._model,
                model_name=self.model_name,
                prompt_args=prompt_args,
            )
            if agent_out.is_complete():
                logger.info("Agent successfully completed task.")
                return

            # Log reflection & script
            logger.info("Reflection:\n%s", agent_out.reflection.strip())
            logger.info("Generated script:\n%s", agent_out.script.rstrip())

            # Execute the script
            execution_result = self._interp.run(agent_out.script)
            logger.info("Execution output (truncated to 4k chars):\n%s", execution_result[:4096])

            # Prepare previousâ€‘turn context for next iteration
            prev_turn = SimpleNamespace(
                reflection=agent_out.reflection,
                script=agent_out.script,
                output=execution_result,
            )

        logger.warning("ðŸ’€ Maximum steps (%d) reached without confirmed exploit.", self.max_steps)

    # ------------------------------------------------------------------
    # Internals
    # ------------------------------------------------------------------

    @staticmethod
    def _init_default_logger() -> logging.Logger:
        logger = logging.getLogger("pentest.session")
        logger.setLevel(logging.DEBUG)

        handler = logging.StreamHandler(sys.stdout)
        handler.setFormatter(
            logging.Formatter("%(asctime)s â€” %(levelname)s â€” %(message)s")
        )
        logger.addHandler(handler)
        return logger

# Tmrw:
# 1. add logging (global)
# 2. create basic eval wrapper and confirm that
# 3. run 15~ labs serially and see if we get *into trouble with portswigger*
# --> actually very important we come to some determination on this tmrw
# 4. debug problem with ActionModel -> need this to work before getting 
# the browser traversal agent working 
# TODO: try mass starting labs and see if Burp stops us
def main(url) -> None:  # pragma: no cover (simple demo runner)
    problem = (
        """
There is a pathâ€‘traversal vulnerability in the following endpoint:

{url}/image?filename=image1.png

Find a payload that exfiltrates /etc/passwd or similar file
"""
    ).format(url=url)

    session = PentestSession(
        problem_stmt=problem,
        # TODO: this success condition should go into PORTSWIGGER_JSON
        success_condition="You have successfully exfiltrated the contents of /etc/passwd", 
        max_steps=8)
    session.run()


if __name__ == "__main__":
    import sys
    from .logger import setup_agent_logger
    
    setup_agent_logger("path_traversal_2", name="agentlog")

    with open(PORTSWIGGER_JSON, "r") as f:
        labs = json.load(f)

    lab_url = sys.argv[1]
    main(lab_url)
