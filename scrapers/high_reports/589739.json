{
  "reported_to": "Node.js",
  "reported_by": "jasnell ",
  "title": "Multiple HTTP/2 DOS Issues",
  "content": "\nA security researcher has conducted a broad survey of HTTP/2 implementations to investigate common Denial of Service attack vectors. The Node.js implementation has been found to be subject to a number of these issues. (On the plus side, we're not the only ones! ;-) ...)\nThis work is still under embargo and has not yet been disclosed.\nSpecifically:\nData Dribble Attack: \"This program will request 1MB of data from a specified resource. It will request this same resource over 100 streams (so, 100MB total). It manipulates window sizes and stream priority to force the server to queue the data in 1-byte chunks.\"\nPing Flood (nginx variant): \"Nginx and libnghttp2 (used by Apache, Tomcat, node.js, and others) has a 10K-message limit on the number of control messages it will queue. Sending a controlled number of messages may enable an attacker to force the server to hold 10K messages in memory...\"\nResource Loop: \"(actually, it should be called \u201cPriority Shuffling\u201d): This program continually shuffles the priority of streams in a way which causes substantial churn to the priority tree. Node.js [is] particularly impacted.\"\nReset Flood: \"This opens a number of streams and sends an invalid request over each stream. In some servers, this solicits a string of stream RSTs. In [Node.js] the servers may queue the RSTs internally until they run out of memory.\"\nO-Length Headers Leak: \"This sends a stream of headers with a 0-length header name and 0-length header value. [Node.js] allocates memory for these headers and keeps the allocation alive until the session dies. Because the names and values are 0 bytes long, the cumulative length never exceeds the header size limit.\"\nInternal Data Buffering: \"This opens the HTTP/2 window so the server can send without constraint; however, it leaves the TCP window closed so the server cannot actually write (many of) the bytes on the wire. Then, the client sends a stream of requests for a large response object which the target queues internally. This appears to work to create a long-ish standing queue in node.js\"\nEach is a distinct issue that will need to be looked at individually. I've edited the descriptions to remove references to vulnerabilities in other HTTP/2 implementations that have not yet been disclosed.\n\nAdditional details from the report:\nCode 3.48 KiB\n1\u201cData Dribble\u201d on node.js: node.js seems to queue the data internally. For a 1MB output file\n2requested 100 times in parallel fast enough that node.js is constantly processing input,\n3node.js\u2019s RSS rises by 808MB and then falls by 120MB (for an aggregate rise of 688MB).\n4(Actually, it looks like the numbers vary a bit across tests, but I think the end result is \u201ca lot\u201d.)\n5However, node.js does not have the excess CPU utilization which Nginx exhibits. If you\n6instead delay the sends considerably so that node.js has time to try to send in the meantime, it\n7looks like node.js will kill off the session before the input queue grows more than a few\n8hundred MB.\n9\n10\u201cInternal Data Buffering\u201d on node.js: For a 1MB output file requested 100 times in parallel\n11(but sent with 24 requests per SSL frame), node.js behaves in an interesting way. It appears to\n12buffer some, but not all, data internally. It seems to continue reading (and processing requests\n13and queueing data to satisfy those requests) for as many streams as it can until it can\u2019t read\n14any more. Once it can\u2019t read anymore, it appears to try to write and realize the writing is\n15blocked. At that point, it seems to switch to reading frames from the wire and queuing the\n16requests internally (without processing them). (All of this is conjecture and is based on what\n17I\u2019ve observed rather than a detailed analysis of the code.) So, if you pack the 100 requests\n18into a single SSL frame, node.js\u2019s RSS increases by approximately 246MB. Or, if you send\n19585 requests in a single SSL frame, node.js\u2019s RSS increases by approximately 1,296MB. For\n20reasons that are not entirely clear to me, if you send 100K requests each on three different\n21connections (approximately 2.8MB of request data per connection, node.js will run out of\n22memory and crash. The other interesting thing that happens is on the session ending. When\n23the session ends, it looks like node.js temporarily starts reading everything which is left in the\n24input queue, tries to process the requests, and store the request output in memory. So,\n25sending 100,000 requests (approximately 2.8MB of request data) and then closing the\n26connection can make node.js temporarily use 12GB of RSS.\n27\n28Resource Loop on node.js: Over the loopback interface, node.js can handle roughly ~10 Mb/s\n29before the assigned thread uses 100% of its CPU core (on an m5.24xlarge). RSS rose from\n3050MB at the start of the test to 236MB by the end of test (~3 minutes). RSS rose another\n31156MB when a second stream was added. With two streams, serving of content to another\n32(non-attacking) connection was severely impacted.\n33\n34Zero-length Headers on node.js: With truly 0-length headers (i.e. the payload is 0 bytes), the\n35server will accept and process an unlimited number; however, they don\u2019t seem to create a\n36standing queue on the server side. The processing overhead is much lighter than the\n37\u201cResource Loop\u201d test. (Roughly 25 Mb/s only produces a 75% CPU load on the server.) With\n380-length headers which are Huffman encoded into 1-byte or greater headers, the server input\n39for that socket (and only that socket) seems to get blocked for ~ 2 minutes, until the\n40connection is killed off. It appears that the server will hold the connection open even if the\n41client goes away. That behavior allows a different kind of DoS attack (exhaust server file\n42descriptors or kernel receive buffers).\n43\n44Reset Flood on node.js: The server queue grows without an obvious bound until the\n45connection dies or the server runs out of memory and dies. After the connection ends, the\n46server is unresponsive while GC runs\nImpact\nMultiple denial of service vectors.\n\n",
  "severity": [
    7.0,
    8.9
  ],
  "bounty": null,
  "weaknesses": [
    "Uncontrolled Resource Consumption"
  ],
  "screenshots": {},
  "disclosed_date": 1566013200,
  "report_url": "https://hackerone.com/reports/589739",
  "is_multi_component": true,
  "complexity": "HIGH",
  "novelty": "HIGH",
  "vuln_category": "CODE",
  "steps": [
    [
      1,
      "Data Dribble Attack: Request 1MB of data from a specified resource over 100 streams, manipulating window sizes and stream priority to force the server to queue data in 1-byte chunks."
    ],
    [
      2,
      "Ping Flood: Send a controlled number of control messages (up to 10K) to force the server to hold messages in memory."
    ],
    [
      3,
      "Resource Loop: Continually shuffle the priority of streams to cause substantial churn to the priority tree."
    ],
    [
      4,
      "Reset Flood: Open multiple streams and send invalid requests over each to solicit a string of stream RSTs, causing the server to queue RSTs until it runs out of memory."
    ],
    [
      5,
      "O-Length Headers Leak: Send headers with 0-length names and values, causing the server to allocate memory for these headers until the session dies."
    ],
    [
      6,
      "Internal Data Buffering: Open the HTTP/2 window to allow the server to send without constraint while leaving the TCP window closed, then send a stream of requests for a large response object to queue internally."
    ]
  ],
  "vuln_description": "Multiple HTTP/2 Denial of Service (DOS) vulnerabilities in Node.js, including Data Dribble, Ping Flood, Resource Loop, Reset Flood, O-Length Headers Leak, and Internal Data Buffering. These vulnerabilities allow an attacker to exhaust server resources, cause memory leaks, or disrupt service through various manipulation techniques.",
  "reason": "The vulnerabilities involve multiple components of the HTTP/2 protocol, requiring deep understanding of protocol mechanics, subtle interactions between components, and manipulation of complex states. The novel logic in each attack vector is not commonly found in other applications, making discovery non-trivial.",
  "new_complexity": "VERY_HIGH",
  "requires_code": true,
  "requires_CVE": false,
  "is_ctf": false,
  "other_report": null
}