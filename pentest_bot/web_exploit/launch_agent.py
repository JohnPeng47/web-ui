from __future__ import annotations

import concurrent.futures
import logging
import os
import signal
import sys
from abc import ABC
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Sequence, Tuple, Type, TypeVar, cast

from pydantic import BaseModel
from pentest_bot.web_exploit.agent import PentestSession
from pentest_bot.db import get_session
from pentest_bot.db.tables.exploit_agent import (
    AgentStepORM as AgentStepORM
)
from pentest_bot.db.tables.exploit_agent import (
    create_agent_steps,
    create_pentest_result,
    create_run,
    run_has_results,
)
from pentest_bot.db.tables.exploit_agent import (
    delete_run as delete_run_record,
)
from logger import (
    create_log_dir_or_noop,
    get_agent_loggers,
    run_id_dir,
    setup_agent_logger,
)
from src.llm_models import LLMHub
from src.utils import set_ctxt_id

from pentest_bot.models.steps import AgentStep, StepState as LLMStep
from pentest_bot.web_exploit.prompts.classify_steps import LabelAgentSteps
from .tools import PythonInterpreter

agent_log, full_log = get_agent_loggers()


class LabInfo(BaseModel):
    """The generic class to represents arguments passed into lab"""

    url: str
    name: str
    lab_ind: int
    iters: int = 1

T = TypeVar("T")
LOGGER_NAME = "agentlog"
MAX_OUTPUT_LOG_LEN = 8192  # characters

class LaunchPentestBots(ABC):
    """
    • Creates a shared parent log dir
    • Spawns a ThreadPoolExecutor
    • Runs one agent per lab in parallel
    • Collects & prints results
    """

    def __init__(
        self,
        *,
        lab_urls: Sequence[LabInfo],
        llm_config: Dict,
        max_steps: int = 12,
        include_description: bool = False,
        log_subfolder: str = "pentest_bot",
        is_eval: bool = False,
        comment: str | None = None,
        interp_cls: Type[PythonInterpreter] = PythonInterpreter,
        label_steps: bool = False,
    ):
        self.lab_urls = lab_urls
        self.max_steps = max_steps
        self.include_description = include_description
        self.comment = comment
        self.interp_cls = interp_cls
        self.label_steps = label_steps

        # shared parent directory for **all** logs in this run
        parent_dir = create_log_dir_or_noop(subfolder=log_subfolder)
        self.parent_dir = run_id_dir(parent_dir)
        self.parent_dir.mkdir(parents=True, exist_ok=True)

        self._executor: concurrent.futures.ThreadPoolExecutor | None = None
        self.run_id = self._create_run_record(parent_dir, is_eval)

        # initiate models
        self.llm_config = llm_config
        self.model_router = LLMHub(llm_config["model_config"])

    # ------------------------------------------------------------------ #
    # Database helpers                                                   #
    # ------------------------------------------------------------------ #
    def _create_run_record(self, parent_dir: Path, is_eval: bool) -> int:
        with get_session() as db:
            run_id = create_run(
                db,
                comment=self.comment,
                parent_logdir=str(parent_dir),
                is_eval=is_eval,
            )
            self.run_id = run_id
            return run_id

    def _delete_runs(self, force: bool = False) -> None:
        """Delete runs"""
        if force:
            with get_session() as db:
                has_results = run_has_results(db, self.run_id)
                if force or not has_results:
                    delete_run_record(db, self.run_id)

    def _create_results_record(
        self,
        eval_name: str,
        success: bool,
        steps: int,
        max_steps: int,
        model_name: str,
        model_costs: float,
        log_filepath: str,
        opik_prompt_name: str,
        opik_prompt_commit: str,
    ) -> int | None:
        """Insert a single row into *pentest_results*.

        The function is deliberately minimal – it just opens a new session,
        writes the record, and closes the session.  Error handling is kept
        intentionally broad because failed persistence must **never** crash
        the running agents.

        Returns:
            The ID of the newly created PentestResult record, or None if creation failed.
        """
        try:
            with get_session() as db:
                result_id = create_pentest_result(
                    db,
                    run_id=cast(int, self.run_id),
                    eval_name=eval_name,
                    success=success,
                    steps=steps,
                    max_steps=max_steps,
                    model_name=model_name,
                    model_costs=model_costs,
                    log_filepath=log_filepath,
                    opik_prompt_name=opik_prompt_name,
                    opik_prompt_commit=opik_prompt_commit,
                )
                return result_id
        except Exception:  # noqa: BLE001 – log & swallow
            logging.getLogger(LOGGER_NAME).exception(
                "Failed to persist results for %s", eval_name
            )

    def _create_agent_steps_record(
        self,
        agent_steps: List[AgentStep],
        result_id: int,
        opik_prompt_name: str | None,
        opik_prompt_commit: str | None,
        step_types: List[LLMStep] | None,
    ) -> None:
        """Insert a single row into *agent_steps*."""
        try:
            with get_session() as db:
                create_agent_steps(
                    db,
                    agent_steps,
                    result_id,
                    opik_prompt_name=opik_prompt_name,
                    opik_prompt_commit=opik_prompt_commit,
                    step_types=step_types,
                )
        except Exception:  # noqa: BLE001 – log & swallow
            agent_log.exception("Failed to persist agent steps for %s", self.run_id)

    def _install_sigint_handler(self) -> None:
        def _sigint(signum, frame):
            print("\n[!] SIGINT received – terminating thread pool …", file=sys.stderr)
            if self._executor:
                self._executor.shutdown(wait=False, cancel_futures=True)

            # delete all runs if sigint forces us
            self._delete_runs(force=True)
            logging.shutdown()
            os._exit(1)

        signal.signal(signal.SIGINT, _sigint)

    def start_pentest_session(self, lab_info: LabInfo):
        """Starts a pentest session for a single lab"""
        raise NotImplementedError("Subclass must implement this method")

    def start_agent(
        self, lab_info: LabInfo, thread_id: int
    ) -> Tuple[LabInfo, Optional[Path]]:
        """
        Launches a single PentestSession for one lab.
        Signature identical to the old free function (model & dirs taken
        from self).
        """
        try:
            set_ctxt_id(str(lab_info.lab_ind))  # per-lab context ID

            # pass a separate instance of model to each agent so we can track costs separately
            model_router = LLMHub(self.llm_config["model_config"])

            _, log_filepath = setup_agent_logger(
                "agent_log",
                parent_dir=self.parent_dir,
            )
            agent_log = logging.getLogger("agentlog")
            agent_log.info("agent started for %s", lab_info.name)

            session = self.start_pentest_session(lab_info)
            success, steps, max_steps, model_name = session.result()
            agent_steps = session.steps()

            self.persist_results(
                lab_info.name, 
                log_filepath, 
                success, 
                steps, 
                max_steps, 
                model_name, 
                agent_steps, 
                session,
                model_router
            )
            return lab_info, log_filepath

        except Exception:
            logging.getLogger("agentlog").exception(
                "Uncaught error in %s", lab_info.name
            )
            return lab_info, None

    def persist_results(
        self, 
        lab_name: str, 
        log_filepath: Path, 
        success: bool, 
        steps: int, 
        max_steps: int, 
        model_name: str, 
        agent_steps: List[AgentStep], 
        session: PentestSession,
        model_router: LLMHub
    ) -> None:
        result_opik_prompt_name, result_opik_prompt_commit = (
            session.opik_prompt_info()
        )
        result_id = self._create_results_record(
            lab_name,
            success,
            steps,
            max_steps,
            model_name,
            # we sum costs once both the critic and the agent are done
            sum([v for _, v in model_router.get_costs().items()]),
            str(log_filepath),
            result_opik_prompt_name,
            result_opik_prompt_commit,
        )

        if self.label_steps:
            model = model_router.get("classify-steps")
            step_labeler = LabelAgentSteps(opik_config=self.llm_config["prompt_config"]["classify-steps"])
            step_types = step_labeler.invoke(
                model, prompt_args={"results": agent_steps}
            )
            step_labeler_opik_prompt_name, step_labeler_opik_prompt_commit = (
                step_labeler.get_opik_prompt_info()
            )
        else:
            step_types = None
            step_labeler_opik_prompt_name = None
            step_labeler_opik_prompt_commit = None
        
        self._create_agent_steps_record(
            agent_steps,
            result_id,
            step_labeler_opik_prompt_name,
            step_labeler_opik_prompt_commit,
            step_types=step_types.steps if step_types else None,
        )

    def start_labs(self) -> None:
        self._install_sigint_handler()
        self._executor = concurrent.futures.ThreadPoolExecutor()

        lab_paths = defaultdict(list)
        futures = [
            self._executor.submit(self.start_agent, lab, thread_id)
            for thread_id, lab in enumerate(self.lab_urls, start=1)
        ]

        try:
            while futures:
                done, futures = concurrent.futures.wait(
                    futures,
                    timeout=0.25,
                    return_when=concurrent.futures.FIRST_COMPLETED,
                )

                # ── NEW: collect results as futures complete ───────────────
                for fut in done:
                    try:
                        lab_info, log_path = fut.result()
                        if log_path is not None:
                            lab_paths[lab_info.lab_ind].append(log_path)
                    except Exception:
                        logging.getLogger("agentlog").exception(
                            "Future processing failed"
                        )
        except KeyboardInterrupt:
            sys.exit()
        finally:
            self._executor.shutdown(wait=False)
            # Flush & close all logging handlers to release file locks (especially on Windows).
            logging.shutdown()