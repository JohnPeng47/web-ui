from __future__ import annotations

from typing import Callable, Dict, List, Optional, Sequence, Tuple, Type, TypeVar, cast

from opik import Opik as opik_client
from pydantic import BaseModel, ValidationError
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from pentest_bot.db.tables.exploit_agent import (
    AgentStepORM as AgentStepORM,
)
from pentest_bot.db.tables.exploit_agent import (
    PentestResultORM as PentestResultORM,
)
from pentest_bot.db.tables.exploit_agent import (
    RunORM as RunORM,
)
from logger import (
    get_agent_loggers,
)
from pentest_bot.web_exploit.prompts.update import PROMPTS_TO_TRACK
from src.llm_models import ChatModelWithName, LLMHub
from src.llm_provider import extract_json, get_instructor_prompt

from pentest_bot.models.steps import AgentContext, AgentStep, AgentStepLM, StepState as LLMStep
from .tools import PythonInterpreter, SideCarLMP

agent_log, full_log = get_agent_loggers()


class LabInfo(BaseModel):
    """The generic class to represents arguments passed into lab"""

    url: str
    name: str
    lab_ind: int
    iters: int = 1


T = TypeVar("T")
LOGGER_NAME = "agentlog"
MAX_OUTPUT_LOG_LEN = 8192  # characters
DEFAULT_HIST_KEYS = ["reflection", "execution_output"]


class ThinkTool(SideCarLMP):
    prompt = """
{trace}

You are auditing the trace of a pentesting agent's execution above
You want to ensure that the agent is:
- On a reasonable trajectory towards uncovering a potential vuln within the past few actions its has taken
- Realign the agent if you find that they are exploring a fruitless path
- Iterate and reflect on the agent's past actions, and whether any key observations have been missed

It may be helpful to include references to actions/observations from previous steps, with the *explicit* step number in your guidance

Output your audit-guidance-reflection in the following format:
- Summarizes in bullet points the past actions
- Center on the current situation
- Decide whether to:
--> Maintain the current trajectory
--> Maintain the overall hypothesis but take a slightly different approach
--> Abandon the current hypothesis and take a completely different approach
"""

    def __init__(
        self,
        model: ChatModelWithName,
        include_keys: List[str] = [],
    ):
        self.include_keys = include_keys
        self.model = model

    def _hist_to_prompt_str(self, agent_ctxt: List[AgentStep]) -> str:
        result_lines = []
        for i, agent_result in enumerate(agent_ctxt):
            for field_name in self.include_keys:
                if hasattr(agent_result, field_name):
                    field_value = getattr(agent_result, field_name)
                    result_lines.append(f"[Step {i}] {field_name}: {field_value}")

        return "\n".join(result_lines)

    def get_reflection(self, agent_ctxt: List[AgentStep]) -> str:
        ctxt_str = self._hist_to_prompt_str(agent_ctxt)
        # agent_log.info("THINK_PROMPT: %s", self.prompt.format(trace=ctxt_str))
        return self.model.invoke(self.prompt.format(trace=ctxt_str)).content

# TODO:
# - Add failure logging:
#   - log agent failures due to pydantic parsing (is there another way it fails?)
# - Managing Agent Context:
#   - managing agent context in general
#   - for long running agents need to compact logs
# - Benchmark Browser:
#   - compare chromium, chrome, browser-use with the flags
#   - local benchmarking; may need to use pool of browsers locally, to cap resource consumption
# which also lets use adapt to code request from a remote browser instance
# - Logging:
#   - rewrite logging so that we can come up with custom logging folder structure
# - Lab Start Interface:
#   - consolidate a common interface for Docker + regular labs
class PentestSession:
    """Runs the LLM ↔ interpreter feedback loop until success or exhaustion."""
    def __init__(
        self,
        *,
        model_config: Dict,
        prompt_config: Dict,
        problem_stmt: str,
        interpreter: PythonInterpreter,
        max_steps: int = 12,
        eval_fn: Optional[Callable[..., bool]] = None,
        use_external_think: bool = False,
        agent_steps: Optional[List[AgentStep]] = None,
    ) -> None:
        self.problem_stmt = problem_stmt
        self.max_steps = max_steps
        self._is_success = eval_fn
        # if not self._is_success:
        #     raise Exception("No eval_fn provided")

        self._interp = interpreter
        self._interp_prompt = self._interp.get_interp_modules_prompt()

        try:
            opik_prompt = opik_client().get_prompt(prompt_config["name"], commit=prompt_config.get("commit", None))
            if not opik_prompt:
                raise ValueError(f"No opik prompt found for {prompt_config['name']}")
            self._sys_prompt = opik_prompt.prompt
            self._opik_prompt_name = opik_prompt.name
            self._opik_prompt_commit = opik_prompt.commit
        except Exception as e:
            opik_prompt = PROMPTS_TO_TRACK.get(prompt_config["name"])
            self._sys_prompt = opik_prompt.prompt
            self._opik_prompt_name = opik_prompt.name
            self._opik_prompt_commit = opik_prompt.commit

        # if use_external_think:
        #     self._think_tool = ThinkTool(
        #         model=model.get("think"),
        #         include_keys=["reflection", "execution_output", "script"],
        #     )

        # models
        self._model = LLMHub(model_config)
        self._agent_model = self._model.get("agent")

        # Session‑level state
        if agent_steps is None:
            agent_steps = []
        self._scratchpad = ""
        self._agent_ctxt: AgentContext = AgentContext(agent_steps.copy())
        self._success: bool = False
        self._completed_steps: int = 0

    def _build_agent_prompt(self) -> str:
        """Inject *task_prompt* into the shared Jinja template returned to the LLM."""
        last_agent_step = self._agent_ctxt.prev_step()
        prompt_str = ""
        prompt_str += f"""
{self._sys_prompt}

<problem_stmt>
Here is your task:
{self.problem_stmt}
</problem_stmt>
"""
        prompt_str += (
            f"""
<prev_reflection>
Here is the reflection from the previous action:
{last_agent_step.reflection}
</prev_reflection>
"""
            if last_agent_step
            else ""
        )

        prompt_str += (
            f"""
<prev_script>
Here is the generated script:
{last_agent_step.script}
</prev_script>
"""
            if last_agent_step
            else ""
        )

        prompt_str += (
            f"""
<prev_output>
Here is the output from the previous action:
{last_agent_step.execution_output}
</prev_output>
"""
            if last_agent_step
            else ""
        )

        prompt_str += (
            f"""
<agent_history>
Here is the history of the agent's actions:
{self._agent_ctxt.history()}
</agent_history>
"""
            if last_agent_step
            else ""
        )

        prompt_str += (
            f"""
<scratchpad>
Scratchpad (context from previous turns):
{self._scratchpad.strip()}
</scratchpad>
"""
            if self._scratchpad.strip()
            else ""
        )

        prompt_str += (
            f"""
<interp_prompt>
{self._interp_prompt}
</interp_prompt>
"""
            if self._interp_prompt
            else ""
        )
        return prompt_str

    def _update_agent_state(
        self, agent_step: AgentStepLM, execution_result: str
    ) -> None:
        self._agent_ctxt.update(
            AgentStep(
                step_num=self._completed_steps + 1,
                reflection=agent_step.reflection,
                script=agent_step.script,
                execution_output=execution_result,
            )
        )
        self._scratchpad += agent_step.add_to_scratchpad
        self._completed_steps += 1

        if self._is_success and self._is_success():
            self._success = True

    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type(ValidationError),
        reraise=True,
    )
    def _invoke(self, prompt_str: str) -> AgentStepLM:
        try:
            res = self._agent_model.invoke(prompt_str)
            res_json = extract_json(res.content)
            return AgentStepLM.model_validate_json(res_json)
        except ValidationError as e:
            agent_log.info(f"Error validating response: {e}")
            agent_log.info(
                f"Raw Response:\n -------------\n{res.content}\n -------------"
            )
            agent_log.info(
                f"Extracted Response:\n -------------\n{res_json}\n -------------"
            )
            raise e

    def _log(self, msg: str) -> None:
        """Default logger logs to both"""
        agent_log.info(msg)
        full_log.info(msg)

    def _log_agent_step(self, agent_step: AgentStepLM, script_out: str) -> None:
        self._log("Reflection:\n%s" % agent_step.reflection.strip())
        full_log.info("Generated script:\n%s" % agent_step.script.rstrip())
        if self._scratchpad:
            self._log("Scratchpad:\n%s" % self._scratchpad.strip())

        full_log.info("Execution output:\n%s", script_out)
        agent_log.info(
            "Execution output (truncated to %d chars):\n%s",
            MAX_OUTPUT_LOG_LEN,
            script_out[:MAX_OUTPUT_LOG_LEN],
        )

    def _add_to_scratchpad(self, text: str, turn: int) -> None:
        self._scratchpad += f"[Turn {turn}] {text.strip()}\n"

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def run(self) -> None:  # noqa: D401 – imperative mood
        """Drive the conversation loop."""

        for step in range(1, self.max_steps + 1):
            self._log("========== STEP %d ==========" % step)

            if step == 1:
                self._log("Prompt:\n%s" % self._build_agent_prompt())

            # TODO: rename to agent context and track token count
            # TODO: this is hard fail, balloons context insanely
            # think_critic = None
            # if step % 6 == 0:
            #     if self._think_tool:
            #         think_critic = self._think_tool.get_reflection(self._agent_results)
            prompt_str = self._build_agent_prompt()
            prompt_str += get_instructor_prompt(AgentStepLM)

            # self._log("Prompt:\n%s" % prompt_str)

            if step % 4 == 0:
                self._log("Prev Reflections: \n%s" % self._agent_ctxt.history())

            # ----------------------------------------------------------------
            # LLM invocation
            # ----------------------------------------------------------------
            agent_step: AgentStepLM = self._invoke(prompt_str)
            # ----------------------------------------------------------------
            # Script execution
            # ----------------------------------------------------------------
            execution_result = self._interp.run(agent_step.script)
            self._log_agent_step(agent_step, execution_result)
            self._update_agent_state(agent_step, execution_result)

            if self._success:
                self._log("Successfully exploited the target!")
                return
        else:
            self._log(
                "💀 Maximum steps (%d) reached without confirmed exploit."
                % self.max_steps
            )

    def result(self) -> Tuple[bool, int, int, str]:  # noqa: D401 – imperative mood
        """Return ``(success, steps_taken, max_steps, model_name)`` for the session."""

        # TODO: add opik prompt commit here
        return (
            self._success,
            self._completed_steps,
            self.max_steps,
            self._agent_model.model_name,
        )

    def steps(self) -> List[AgentStep]:
        return self._agent_ctxt._ctxt

    def opik_prompt_info(self) -> Tuple[str, str]:
        return self._opik_prompt_name, self._opik_prompt_commit