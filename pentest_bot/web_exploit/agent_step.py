from typing import List, Dict, Optional, TYPE_CHECKING
import enum

from pydantic import BaseModel

from src.llm_models import openai_o3_mini
from src.llm_provider import LMP, LMPVerificationException

if TYPE_CHECKING:
    from pentest_bot.db import AgentStepORM

class StepState(str, enum.Enum):
    PROBING = "PROBING"
    SUSPECT = "SUSPECT"
    PROMISING = "PROMISING"
    SOFT_PIVOT = "SOFT_PIVOT"
    HARD_PIVOT = "HARD_PIVOT"

class AgentStepLM(BaseModel):
    """LLM response schema expected by :pymeth:`PentestBot.invoke`."""

    reflection: str
    script: str
    add_to_scratchpad: str = ""

    def is_complete(self) -> bool:  # noqa: D401 – imperative mood
        """Return *True* iff the agent signalled task completion."""

        return "TASK_COMPLETE" in self.script or "TASK_COMPLETE" in self.reflection
    
class AgentStep(AgentStepLM):
    step_num: int
    execution_output: str
    step_type: Optional[StepState] = None

    @classmethod
    def from_db(cls, agent_step: "AgentStepORM") -> "AgentStep":
        return cls(
            step_num=agent_step.step_number,
            reflection=agent_step.reflection,
            script=agent_step.script,
            execution_output=agent_step.execution_output,
            step_type=agent_step.step_type,
        )

    def result(self) -> str:
        return f"Step {self.step_num}: {self.reflection}\nScript Output: {self.execution_output}"


class AgentContext:
    """The main interface for interacting agent step history"""
    def __init__(self, agent_steps: List[AgentStep]):
        self._ctxt = agent_steps
        self._curr_step = 0

    @classmethod
    def from_db(cls, agent_steps: List["AgentStepORM"]) -> "AgentContext":
        return cls([AgentStep.from_db(s) for s in agent_steps])
    
    def update(self, agent_step: AgentStep) -> None:
        self._ctxt.append(agent_step)

    def steps(self) -> List[AgentStep]:
        return self._ctxt

    def prev_step(self) -> AgentStep | None:
        if not self._ctxt:
            return None
        return self._ctxt[-1]
    
    def history(self, step: int = -1) -> str:
        return "\n".join([f"[Step {i}] {step.reflection}" for i, step in enumerate(self._ctxt[:step])])


CRITIC_PROMPT = """
{trace}

Above is the execution trace of pentesting agent. Your task is provide a summary of the trace.
You output the following:
- Group together agent steps in which a consistent goal is being pursued
- Identify anything that the agent finds interesting, and the context around it
- Identify the following errors or mistakes if they occur:
--> The agent makes an incorrect inference from a previous observation
--> The agent repeatedly stuck in a loop over an action that has low EV for finding an exploitable vulnerability

Be terse and concise in your output, but make sure to not omit key information
Finally, make a determination if the agent should continue investigating along its current path
"""

class TransitionGraph:
    pass

class Step(BaseModel):
    step_num: int
    type: StepState

class StepList(BaseModel):
    num_steps: int
    steps: List[Step]

    def __str__(self):
        return "\n".join([f"{step.step_num}: {step.type}" for step in self.steps])

# TODO: error status should be automatically detected from the environment
# Can we write an interface for doing this kind of labeling task?
# Then we can can collect the most of this data 
class LabelAgentSteps(LMP):
    prompt = """
{{trace}}

Above is the execution trace of pentesting agent. Your task is to classify each according to the following step transition scheme:

INVESTIGATION States:
* The investigation 
* In these states, the agent is actively interacting with the web application via scripts
* The agent will take some action each turn via the script 
* Each INVESTIGATION is scoped to a particular part of the application. This "part" is usually something on the scale of a single parameter or a set of query parameters, and should not be as broad as the whole page of an application
PROBING: The agent is searching for presence of a vulnerability but nothing in the application response suggests any anomalous. At this point, exploitation is unlikely
SUSPECT: The initial probing stage has elicited behavior that warrants further investigation. At this point, exploitation is 50/50
PROMISING: After futher investigation, the potential to convert the suspect into a promising vulnerability is high

PIVOT States:
* The current investigative route has proven unfruitful and the agent is pivoting to a new approach
SOFT_PIVOT: the agent is pivoting off of the current investigative track but still targeting the same scope
HARD_PIVOT: the agent is pivoting off of the current investigative track and changing the scope as well

Error States:
ERROR_PYTHON: The program has thrown an exception

Here are a couple of steps that demonstrate the different step labels:
ie. 
- [PROBING] "Collected homepage HTML, identified forms and links with parameters for XSS"
- [SUSPECT] "Reflected input does not escape HTML chars"
- [SUSPECT]  "Injected single quote into ?sort= parameter; server returned SQL error"
- [PROMISING]  "Payload triggered 5 seconds more delay than baseline, query influenced by user input"
- [SOFT_PIVOT]  "Brute-force auth endpoint showed rate limiting; switching to token refresh flow within same login scope"
- [HARD_PIVOT]  "No vulns in account subsystem after 8 steps; shifting investigation to public blog comments feature"
- [HARD_PIVOT]  "Leaving user-profile XSS path; beginning enumeration of file-upload API for unrestricted upload"
* Return a list of the agent steps labeled with the appropriate state
"""
    response_format = StepList

    def _prepare_prompt(self, templates={}, manual_rewrite: bool = False, **prompt_args) -> str:
        results: List[AgentStep] = prompt_args.pop("results")
        results_str = "\n".join([res.result() for res in results])

        prompt_args["trace"] = results_str
        return super()._prepare_prompt(prompt_args)
        
    def _verify_or_raise(self, res: StepList, **prompt_args):
        results: List[AgentStep] = prompt_args.pop("results")

        for ret_step, result_step in zip(res.steps, results):
            if ret_step.step_num != result_step.step_num:
                raise LMPVerificationException
            
if __name__ == "__main__":
    from pentest_bot.db import get_session, get_agent_ctxt, get_run

    with get_session() as session:
        model = openai_o3_mini()
        runs = [r.id for r in get_run(session, 25).results]
        for r in runs:
            steps = get_agent_ctxt(session, r)
            trace = "\n".join([f"{i}: {s.reflection}" for i, s in enumerate(steps, start=1)]) 
            print(trace)
            print(LabelAgentSteps().invoke(model, prompt_args={"results": trace}))
        
        # steps = get_steps_for_result(session, 18)
        # trace = "\n".join([f"{i}: {s.reflection}" for i, s in enumerate(steps, start=1)]) 
        # print(trace)      
        # model = openai_o3_mini()

        # for i in range(3):
        #     print(LabelAgentSteps().invoke(model, prompt_args={"trace": trace}))

        # model_4o = openai_4o()
        # # print("RAW:::")
        # # for i in range(3):
        # #     print(summarize_trace(model, trace))
        # # print("STRUCTURED:::")
        # for i in range(1):
        #     print("---------------o3_mini-----------------")
        #     print(LabelAgentSteps().invoke(model, prompt_args={"trace": trace}))
        #     # print("---------------4o-----------------")
        #     # print(LabelAgentSteps().invoke(model_4o, prompt_args={"trace": trace}))

        # # steps = LabelAgentSteps().invoke(model, prompt_args={"trace": trace})
        # # print(steps)