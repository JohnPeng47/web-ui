from typing import List, Dict, Optional, TYPE_CHECKING
import enum

from pydantic import BaseModel

from src.llm_models import openai_o3_mini
from src.llm_provider import LMP, LMPVerificationException

if TYPE_CHECKING:
    from pentest_bot.db import AgentStepORM

class StepState(str, enum.Enum):
    PROBING = "PROBING"
    SUSPECT = "SUSPECT"
    PROMISING = "PROMISING"
    SOFT_PIVOT = "SOFT_PIVOT"
    HARD_PIVOT = "HARD_PIVOT"
    ERROR = "ERROR"

StepStateAbbr = {
    StepState.PROBING: "P",
    StepState.SUSPECT: "S",
    StepState.PROMISING: "Prm",
    StepState.SOFT_PIVOT: "PS",
    StepState.HARD_PIVOT: "PH",
    StepState.ERROR: "E",
}

class AgentStepLM(BaseModel):
    """LLM response schema expected by :pymeth:`PentestBot.invoke`."""

    reflection: str
    script: str
    add_to_scratchpad: str = ""

    def is_complete(self) -> bool:  # noqa: D401 – imperative mood
        """Return *True* iff the agent signalled task completion."""

        return "TASK_COMPLETE" in self.script or "TASK_COMPLETE" in self.reflection
    
class AgentStep(AgentStepLM):
    step_num: int
    execution_output: str
    step_type: Optional[StepState] = None

    @classmethod
    def from_db(cls, agent_step: "AgentStepORM") -> "AgentStep":
        return cls(
            step_num=agent_step.step_number,
            reflection=agent_step.reflection,
            script=agent_step.script,
            execution_output=agent_step.execution_output,
            step_type=agent_step.step_type,
        )

    # TODO: should deprecate and use agentContext instead to construct prompt_str repr
    def result(self) -> str:
        return f"[Step {self.step_num}]: {self.reflection}\nScript Output: {self.execution_output}"


class AgentContext:
    """The main interface for interacting agent step history"""
    def __init__(self, agent_steps: List[AgentStep]):
        self._ctxt = agent_steps
        self._curr_step = 0

    @classmethod
    def from_db(cls, agent_steps: List["AgentStepORM"]) -> "AgentContext":
        return cls([AgentStep.from_db(s) for s in agent_steps])
    
    def update(self, agent_step: AgentStep) -> None:
        self._ctxt.append(agent_step)

    def steps(self) -> List[AgentStep]:
        return self._ctxt

    def prev_step(self) -> AgentStep | None:
        if not self._ctxt:
            return None
        return self._ctxt[-1]
    
    def history(self, end: Optional[int] = None) -> str:
        return "\n".join([f"[Step {i}] {step.reflection}" for i, step in enumerate(self._ctxt[:end], start=1)])


CRITIC_PROMPT = """
{trace}

Above is the execution trace of pentesting agent. Your task is provide a summary of the trace.
You output the following:
- Group together agent steps in which a consistent goal is being pursued
- Identify anything that the agent finds interesting, and the context around it
- Identify the following errors or mistakes if they occur:
--> The agent makes an incorrect inference from a previous observation
--> The agent repeatedly stuck in a loop over an action that has low EV for finding an exploitable vulnerability

Be terse and concise in your output, but make sure to not omit key information
Finally, make a determination if the agent should continue investigating along its current path
"""

class TransitionGraph:
    pass

class Step(BaseModel):
    step_num: int
    type: StepState

class StepList(BaseModel):
    num_steps: int
    steps: List[Step]

    def __str__(self):
        return "\n".join([f"{step.step_num}: {step.type}" for step in self.steps])

class LabelAgentSteps(LMP):
    prompt = "classify-steps"
    response_format = StepList

    def _prepare_prompt(self, templates={}, manual_rewrite: bool = False, **prompt_args) -> str:
        results: List[AgentStep] = prompt_args.pop("results")
        results_str = "\n".join([res.result() for res in results])

        prompt_args["trace"] = results_str
        return super()._prepare_prompt(prompt_args)
        
    def _verify_or_raise(self, res: StepList, **prompt_args):
        results: List[AgentStep] = prompt_args.pop("results")

        for ret_step, result_step in zip(res.steps, results):
            if ret_step.step_num != result_step.step_num:
                raise LMPVerificationException