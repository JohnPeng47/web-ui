from __future__ import annotations

import concurrent.futures
import logging
import os
import signal
import sys
from abc import ABC, abstractmethod
from collections import defaultdict
from pathlib import Path
from typing import Callable, Dict, List, Optional, Sequence, Tuple, Type, TypeVar, cast

from playwright.sync_api import sync_playwright
from opik import Opik as opik_client
from pydantic import BaseModel, ValidationError
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from pentest_bot.db import get_session
from pentest_bot.db.tables.agent import (
    AgentStepORM as AgentStepORM,
)
from pentest_bot.db.tables.agent import (
    PentestResultORM as PentestResultORM,
)
from pentest_bot.db.tables.agent import (
    RunORM as RunORM,
)
from pentest_bot.db.tables.agent import (
    create_agent_steps,
    create_pentest_result,
    create_run,
    run_has_results,
)
from pentest_bot.db.tables.agent import (
    delete_run as delete_run_record,
)
from pentest_bot.logger import (
    create_log_dir_or_noop,
    get_agent_loggers,
    run_id_dir,
    setup_agent_logger,
)
from pentest_bot.web_exploit.prompts.update import PROMPTS_TO_TRACK
from src.llm_models import ChatModelWithName, LLMHub
from src.llm_provider import extract_json, get_instructor_prompt
from src.utils.context import set_ctxt_id

from pentest_bot.models.steps import AgentContext, AgentStep, AgentStepLM, StepState as LLMStep
from pentest_bot.web_exploit.prompts.classify_steps import LabelAgentSteps
from .tools import PythonInterpreter, SideCarLMP

agent_log, full_log = get_agent_loggers()


class LabInfo(BaseModel):
    """The generic class to represents arguments passed into lab"""

    url: str
    name: str
    lab_ind: int
    iters: int = 1


T = TypeVar("T")
LOGGER_NAME = "agentlog"
MAX_OUTPUT_LOG_LEN = 8192  # characters
DEFAULT_HIST_KEYS = ["reflection", "execution_output"]


class ThinkTool(SideCarLMP):
    prompt = """
{trace}

You are auditing the trace of a pentesting agent's execution above
You want to ensure that the agent is:
- On a reasonable trajectory towards uncovering a potential vuln within the past few actions its has taken
- Realign the agent if you find that they are exploring a fruitless path
- Iterate and reflect on the agent's past actions, and whether any key observations have been missed

It may be helpful to include references to actions/observations from previous steps, with the *explicit* step number in your guidance

Output your audit-guidance-reflection in the following format:
- Summarizes in bullet points the past actions
- Center on the current situation
- Decide whether to:
--> Maintain the current trajectory
--> Maintain the overall hypothesis but take a slightly different approach
--> Abandon the current hypothesis and take a completely different approach
"""

    def __init__(
        self,
        model: ChatModelWithName,
        include_keys: List[str] = [],
    ):
        self.include_keys = include_keys
        self.model = model

    def _hist_to_prompt_str(self, agent_ctxt: List[AgentStep]) -> str:
        result_lines = []
        for i, agent_result in enumerate(agent_ctxt):
            for field_name in self.include_keys:
                if hasattr(agent_result, field_name):
                    field_value = getattr(agent_result, field_name)
                    result_lines.append(f"[Step {i}] {field_name}: {field_value}")

        return "\n".join(result_lines)

    def get_reflection(self, agent_ctxt: List[AgentStep]) -> str:
        ctxt_str = self._hist_to_prompt_str(agent_ctxt)
        # agent_log.info("THINK_PROMPT: %s", self.prompt.format(trace=ctxt_str))
        return self.model.invoke(self.prompt.format(trace=ctxt_str)).content


# TODO:
# - Add failure logging:
#   - log agent failures due to pydantic parsing (is there another way it fails?)
# - Managing Agent Context:
#   - managing agent context in general
#   - for long running agents need to compact logs
# - Benchmark Browser:
#   - compare chromium, chrome, browser-use with the flags
#   - local benchmarking; may need to use pool of browsers locally, to cap resource consumption
# which also lets use adapt to code request from a remote browser instance
# - Logging:
#   - rewrite logging so that we can come up with custom logging folder structure
# - Lab Start Interface:
#   - consolidate a common interface for Docker + regular labs
class PentestSession:
    """Runs the LLM â†” interpreter feedback loop until success or exhaustion."""

    def __init__(
        self,
        *,
        model_config: Dict,
        prompt_config: Dict,
        problem_stmt: str,
        interpreter: PythonInterpreter,
        max_steps: int = 12,
        eval_fn: Optional[Callable[..., bool]] = None,
        use_external_think: bool = False,
        agent_steps: Optional[List[AgentStep]] = None,
    ) -> None:
        self.problem_stmt = problem_stmt
        self.max_steps = max_steps
        self._is_success = eval_fn
        # if not self._is_success:
        #     raise Exception("No eval_fn provided")

        self._interp = interpreter
        self._interp_prompt = self._interp.get_interp_modules_prompt()

        try:
            opik_prompt = opik_client().get_prompt(prompt_config["name"], commit=prompt_config.get("commit", None))
            if not opik_prompt:
                raise ValueError(f"No opik prompt found for {prompt_config['name']}")
            self._sys_prompt = opik_prompt.prompt
            self._opik_prompt_name = opik_prompt.name
            self._opik_prompt_commit = opik_prompt.commit
        except Exception as e:
            opik_prompt = PROMPTS_TO_TRACK.get(prompt_config["name"])
            self._sys_prompt = opik_prompt.prompt
            self._opik_prompt_name = opik_prompt.name
            self._opik_prompt_commit = opik_prompt.commit

        # if use_external_think:
        #     self._think_tool = ThinkTool(
        #         model=model.get("think"),
        #         include_keys=["reflection", "execution_output", "script"],
        #     )

        # models
        self._model = LLMHub(model_config)
        self._agent_model = self._model.get("agent")

        # Sessionâ€‘level state
        if agent_steps is None:
            agent_steps = []
        self._scratchpad = ""
        self._agent_ctxt: AgentContext = AgentContext(agent_steps.copy())
        self._success: bool = False
        self._completed_steps: int = 0

    def _build_agent_prompt(self) -> str:
        """Inject *task_prompt* into the shared Jinja template returned to the LLM."""
        last_agent_step = self._agent_ctxt.prev_step()
        prompt_str = ""
        prompt_str += f"""
{self._sys_prompt}

<problem_stmt>
Here is your task:
{self.problem_stmt}
</problem_stmt>
"""
        prompt_str += (
            f"""
<prev_reflection>
Here is the reflection from the previous action:
{last_agent_step.reflection}
</prev_reflection>
"""
            if last_agent_step
            else ""
        )

        prompt_str += (
            f"""
<prev_script>
Here is the generated script:
{last_agent_step.script}
</prev_script>
"""
            if last_agent_step
            else ""
        )

        prompt_str += (
            f"""
<prev_output>
Here is the output from the previous action:
{last_agent_step.execution_output}
</prev_output>
"""
            if last_agent_step
            else ""
        )

        prompt_str += (
            f"""
<agent_history>
Here is the history of the agent's actions:
{self._agent_ctxt.history()}
</agent_history>
"""
            if last_agent_step
            else ""
        )

        prompt_str += (
            f"""
<scratchpad>
Scratchpad (context from previous turns):
{self._scratchpad.strip()}
</scratchpad>
"""
            if self._scratchpad.strip()
            else ""
        )

        prompt_str += (
            f"""
<interp_prompt>
{self._interp_prompt}
</interp_prompt>
"""
            if self._interp_prompt
            else ""
        )
        return prompt_str

    def _update_agent_state(
        self, agent_step: AgentStepLM, execution_result: str
    ) -> None:
        self._agent_ctxt.update(
            AgentStep(
                step_num=self._completed_steps + 1,
                reflection=agent_step.reflection,
                script=agent_step.script,
                execution_output=execution_result,
            )
        )
        self._scratchpad += agent_step.add_to_scratchpad
        self._completed_steps += 1

        if self._is_success and self._is_success():
            self._success = True

    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type(ValidationError),
        reraise=True,
    )
    def _invoke(self, prompt_str: str) -> AgentStepLM:
        try:
            res = self._agent_model.invoke(prompt_str)
            res_json = extract_json(res.content)
            return AgentStepLM.model_validate_json(res_json)
        except ValidationError as e:
            agent_log.info(f"Error validating response: {e}")
            agent_log.info(
                f"Raw Response:\n -------------\n{res.content}\n -------------"
            )
            agent_log.info(
                f"Extracted Response:\n -------------\n{res_json}\n -------------"
            )
            raise e

    def _log(self, msg: str) -> None:
        """Default logger logs to both"""
        agent_log.info(msg)
        full_log.info(msg)

    def _log_agent_step(self, agent_step: AgentStepLM, script_out: str) -> None:
        self._log("Reflection:\n%s" % agent_step.reflection.strip())
        full_log.info("Generated script:\n%s" % agent_step.script.rstrip())
        if self._scratchpad:
            self._log("Scratchpad:\n%s" % self._scratchpad.strip())

        full_log.info("Execution output:\n%s", script_out)
        agent_log.info(
            "Execution output (truncated to %d chars):\n%s",
            MAX_OUTPUT_LOG_LEN,
            script_out[:MAX_OUTPUT_LOG_LEN],
        )

    def _add_to_scratchpad(self, text: str, turn: int) -> None:
        self._scratchpad += f"[Turn {turn}] {text.strip()}\n"

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def run(self) -> None:  # noqa: D401 â€“Â imperative mood
        """Drive the conversation loop."""

        for step in range(1, self.max_steps + 1):
            self._log("========== STEP %d ==========" % step)

            if step == 1:
                self._log("Prompt:\n%s" % self._build_agent_prompt())

            # TODO: rename to agent context and track token count
            # TODO: this is hard fail, balloons context insanely
            # think_critic = None
            # if step % 6 == 0:
            #     if self._think_tool:
            #         think_critic = self._think_tool.get_reflection(self._agent_results)
            prompt_str = self._build_agent_prompt()
            prompt_str += get_instructor_prompt(AgentStepLM)

            # self._log("Prompt:\n%s" % prompt_str)

            if step % 4 == 0:
                self._log("Prev Reflections: \n%s" % self._agent_ctxt.history())

            # ----------------------------------------------------------------
            # LLM invocation
            # ----------------------------------------------------------------
            agent_step: AgentStepLM = self._invoke(prompt_str)
            # ----------------------------------------------------------------
            # Script execution
            # ----------------------------------------------------------------
            execution_result = self._interp.run(agent_step.script)
            self._log_agent_step(agent_step, execution_result)
            self._update_agent_state(agent_step, execution_result)

            if self._success:
                self._log("Successfully exploited the target!")
                return
        else:
            self._log(
                "ðŸ’€ Maximum steps (%d) reached without confirmed exploit."
                % self.max_steps
            )

    def result(self) -> Tuple[bool, int, int, str]:  # noqa: D401 â€“Â imperative mood
        """Return ``(success, steps_taken, max_steps, model_name)`` for the session."""

        # TODO: add opik prompt commit here
        return (
            self._success,
            self._completed_steps,
            self.max_steps,
            self._agent_model.model_name,
        )

    def steps(self) -> List[AgentStep]:
        return self._agent_ctxt._ctxt

    def opik_prompt_info(self) -> Tuple[str, str]:
        return self._opik_prompt_name, self._opik_prompt_commit


class LaunchPentestBots(ABC):
    """
    â€¢ Creates a shared parent log dir
    â€¢ Spawns a ThreadPoolExecutor
    â€¢ Runs one agent per lab in parallel
    â€¢ Collects & prints results
    """

    def __init__(
        self,
        *,
        lab_urls: Sequence[LabInfo],
        llm_config: Dict,
        max_steps: int = 12,
        include_description: bool = False,
        log_subfolder: str = "pentest_bot",
        is_eval: bool = False,
        comment: str | None = None,
        interp_cls: Type[PythonInterpreter] = PythonInterpreter,
        label_steps: bool = False,
    ):
        self.lab_urls = lab_urls
        self.max_steps = max_steps
        self.include_description = include_description
        self.comment = comment
        self.interp_cls = interp_cls
        self.label_steps = label_steps

        # shared parent directory for **all** logs in this run
        parent_dir = create_log_dir_or_noop(subfolder=log_subfolder)
        self.parent_dir = run_id_dir(parent_dir)
        self.parent_dir.mkdir(parents=True, exist_ok=True)

        self._executor: concurrent.futures.ThreadPoolExecutor | None = None
        self.run_id = self._create_run_record(parent_dir, is_eval)

        # initiate models
        self.llm_config = llm_config
        self.model_router = LLMHub(llm_config["model_config"])

    # ------------------------------------------------------------------ #
    # Database helpers                                                   #
    # ------------------------------------------------------------------ #
    def _create_run_record(self, parent_dir: Path, is_eval: bool) -> int:
        with get_session() as db:
            run_id = create_run(
                db,
                comment=self.comment,
                parent_logdir=str(parent_dir),
                is_eval=is_eval,
            )
            self.run_id = run_id
            return run_id

    def _delete_runs(self, force: bool = False) -> None:
        """Delete runs"""
        if force:
            with get_session() as db:
                has_results = run_has_results(db, self.run_id)
                if force or not has_results:
                    delete_run_record(db, self.run_id)

    def _create_results_record(
        self,
        eval_name: str,
        success: bool,
        steps: int,
        max_steps: int,
        model_name: str,
        model_costs: float,
        log_filepath: str,
        opik_prompt_name: str,
        opik_prompt_commit: str,
    ) -> int | None:
        """Insert a single row into *pentest_results*.

        The function is deliberately minimal â€“ it just opens a new session,
        writes the record, and closes the session.  Error handling is kept
        intentionally broad because failed persistence must **never** crash
        the running agents.

        Returns:
            The ID of the newly created PentestResult record, or None if creation failed.
        """
        try:
            with get_session() as db:
                result_id = create_pentest_result(
                    db,
                    run_id=cast(int, self.run_id),
                    eval_name=eval_name,
                    success=success,
                    steps=steps,
                    max_steps=max_steps,
                    model_name=model_name,
                    model_costs=model_costs,
                    log_filepath=log_filepath,
                    opik_prompt_name=opik_prompt_name,
                    opik_prompt_commit=opik_prompt_commit,
                )
                return result_id
        except Exception:  # noqa: BLE001 â€“ log & swallow
            logging.getLogger(LOGGER_NAME).exception(
                "Failed to persist results for %s", eval_name
            )

    def _create_agent_steps_record(
        self,
        agent_steps: List[AgentStep],
        result_id: int,
        opik_prompt_name: str | None,
        opik_prompt_commit: str | None,
        step_types: List[LLMStep] | None,
    ) -> None:
        """Insert a single row into *agent_steps*."""
        try:
            with get_session() as db:
                create_agent_steps(
                    db,
                    agent_steps,
                    result_id,
                    opik_prompt_name=opik_prompt_name,
                    opik_prompt_commit=opik_prompt_commit,
                    step_types=step_types,
                )
        except Exception:  # noqa: BLE001 â€“ log & swallow
            agent_log.exception("Failed to persist agent steps for %s", self.run_id)

    def _install_sigint_handler(self) -> None:
        def _sigint(signum, frame):
            print("\n[!] SIGINT received â€“ terminating thread pool â€¦", file=sys.stderr)
            if self._executor:
                self._executor.shutdown(wait=False, cancel_futures=True)

            # delete all runs if sigint forces us
            self._delete_runs(force=True)
            logging.shutdown()
            os._exit(1)

        signal.signal(signal.SIGINT, _sigint)

    def start_pentest_session(self, lab_info: LabInfo):
        """Starts a pentest session for a single lab"""
        raise NotImplementedError("Subclass must implement this method")
        
        interpreter = self.interp_cls(shared_globals={})
        interpreter.set_interp_modules_prompt("You should be using the page_goto tool to read the webpage content")
        session = PentestSession(
            model_config=self.llm_config["model_config"],
            prompt_config=self.llm_config["prompt_config"]["agent"],
            problem_stmt="",
            max_steps=self.max_steps,
            eval_fn=None,
            interpreter=interpreter
        )
        session.run()
        return session

    def start_agent(
        self, lab_info: LabInfo, thread_id: int
    ) -> Tuple[LabInfo, Optional[Path]]:
        """
        Launches a single PentestSession for one lab.
        Signature identical to the old free function (model & dirs taken
        from self).
        """
        try:
            set_ctxt_id(str(lab_info.lab_ind))  # per-lab context ID

            # pass a separate instance of model to each agent so we can track costs separately
            model_router = LLMHub(self.llm_config["model_config"])

            _, log_filepath = setup_agent_logger(
                "agent_log",
                parent_dir=self.parent_dir,
            )
            agent_log = logging.getLogger("agentlog")
            agent_log.info("agent started for %s", lab_info.name)

            session = self.start_pentest_session(lab_info)
            success, steps, max_steps, model_name = session.result()
            agent_steps = session.steps()

            self.persist_results(
                lab_info, 
                log_filepath, 
                success, 
                steps, 
                max_steps, 
                model_name, 
                agent_steps, 
                session,
                model_router
            )
            return lab_info, log_filepath

        except Exception:
            logging.getLogger("agentlog").exception(
                "Uncaught error in %s", lab_info.name
            )
            return lab_info, None

    def persist_results(
        self, 
        lab_info: LabInfo, 
        log_filepath: Path, 
        success: bool, 
        steps: int, 
        max_steps: int, 
        model_name: str, 
        agent_steps: List[AgentStep], 
        session: PentestSession,
        model_router: LLMHub
    ) -> None:
        result_opik_prompt_name, result_opik_prompt_commit = (
            session.opik_prompt_info()
        )
        result_id = self._create_results_record(
            lab_info.name,
            success,
            steps,
            max_steps,
            model_name,
            # we sum costs once both the critic and the agent are done
            sum([v for _, v in model_router.get_costs().items()]),
            str(log_filepath),
            result_opik_prompt_name,
            result_opik_prompt_commit,
        )

        if self.label_steps:
            model = model_router.get("classify-steps")
            step_labeler = LabelAgentSteps(opik_config=self.llm_config["prompt_config"]["classify-steps"])
            step_types = step_labeler.invoke(
                model, prompt_args={"results": agent_steps}
            )
            step_labeler_opik_prompt_name, step_labeler_opik_prompt_commit = (
                step_labeler.get_opik_prompt_info()
            )
        else:
            step_types = None
            step_labeler_opik_prompt_name = None
            step_labeler_opik_prompt_commit = None
        
        self._create_agent_steps_record(
            agent_steps,
            result_id,
            step_labeler_opik_prompt_name,
            step_labeler_opik_prompt_commit,
            step_types=step_types.steps if step_types else None,
        )

    def start_labs(self) -> None:
        self._install_sigint_handler()
        self._executor = concurrent.futures.ThreadPoolExecutor()

        lab_paths = defaultdict(list)
        futures = [
            self._executor.submit(self.start_agent, lab, thread_id)
            for thread_id, lab in enumerate(self.lab_urls, start=1)
        ]

        try:
            while futures:
                done, futures = concurrent.futures.wait(
                    futures,
                    timeout=0.25,
                    return_when=concurrent.futures.FIRST_COMPLETED,
                )

                # â”€â”€ NEW: collect results as futures complete â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                for fut in done:
                    try:
                        lab_info, log_path = fut.result()
                        if log_path is not None:
                            lab_paths[lab_info.lab_ind].append(log_path)
                    except Exception:
                        logging.getLogger("agentlog").exception(
                            "Future processing failed"
                        )
        except KeyboardInterrupt:
            sys.exit()
        finally:
            self._executor.shutdown(wait=False)
            # Flush & close all logging handlers to release file locks (especially on Windows).
            logging.shutdown()