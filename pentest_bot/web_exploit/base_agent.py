from __future__ import annotations

"""Refactored PentestBot driver.

The original script mixed promptâ€‘building, agent orchestration, and sandbox
execution logic in a single, monolithic file.  This rewrite introduces:

* Clear moduleâ€‘level separation of concerns
* Consistent logging and typing throughout
* Consolidated configuration constants
* Defensive error handling with early exits
* PEPâ€‘8 compliant spacing and double quotes
* Explicit public API surface (`run()` and `result()`)

The code remains a single file for ease of distribution, but the internal
layout mirrors how you would structure the project if/when you split it into
multiple modules (``prompts.py``, ``interpreter.py``, etc.).
"""

import concurrent.futures
import shutil
import json
import logging
import os
import re
import signal
import sys
from abc import ABC, abstractmethod
from dataclasses import dataclass
from io import StringIO
from pathlib import Path
from types import SimpleNamespace
from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, TypeVar, Type, cast
from collections import defaultdict

from playwright.sync_api import sync_playwright
from pydantic import BaseModel

from src.llm_models import LLMHub
from src.llm_provider import LMP
from src.utils.context import set_ctxt_id

from scripts.portswigger.data import PORT_SWIGGER_LABS, PROMPT_TEMPLATES, XSS_APPRENTICE_LABS  # type: ignore â€“ projectâ€‘specific
from pentest_bot.logger import get_agent_loggers, setup_agent_logger, create_log_dir_or_noop, run_id_dir

from .agent_step import LabelAgentSteps, StepList, AgentResult, AgentStep, Step as LLMStep
from .xss_client import register_xss_payload
from .tools import PythonInterpreter
from .prompts import AGENT_PROMPT_XSS as AGENT_PROMPT, AGENT_CRITIQUE_PROMPT

from pentest_bot.db import (
    init_db, 
    SessionLocal, 
    PentestResult as PentestResultORM, 
    Run as RunORM, 
    AgentStep as AgentStepORM
)

class LabInfo(BaseModel):
    """The generic class to represents arguments passed into lab"""
    url: str
    name: str
    lab_ind: int
    iters: int = 1

T = TypeVar("T")
LOGGER_NAME = "agentlog"
MAX_OUTPUT_LOG_LEN = 8192  # characters

agent_log, full_log = get_agent_loggers()

# TODO:
# - BUG: sometimes agent will just fail without error, see: logs\discovery_agent\2025-07-14\7\2\agent_log.log
# - Save all logs not just evals
# - Agent Reflection Compaction:
#   - for long running agents need to compact logs
# - Benchmark Browser:
#   - compare chromium, chrome, browser-use with the flags
#   - local benchmarking; may need to use pool of browsers locally, to cap resource consumption
# which also lets use adapt to code request from a remote browser instance
# - Logging:
#   - rewrite logging so that we can come up with custom logging folder structure
# - Lab Start Interface:
#   - consolidate a common interface for Docker + regular labs
def build_pentest_prompt(agent_prompt: str) -> str:
    """Inject *task_prompt* into the shared Jinja template returned to the LLM."""

    return f"""
{{% if problem_stmt %}}
<problem_stmt>
Here is the CTF problem statement:
{{{{ problem_stmt }}}}
</problem_stmt>
{{% endif %}}

{{% if prev_turn %}}
<prev_reflection>
Here is the reflection from the previous action:
{{{{ prev_turn.reflection }}}}
</prev_reflection>

<prev_script>
Here is the generated script:
{{{{ prev_turn.script }}}}
</prev_script>

<prev_output>
Here is the output from the previous action:
{{{{ prev_turn.output }}}}
</prev_output>
{{% endif %}}

{{% if scratchpad %}}
Scratchpad (context from previous turns):
<scratchpad>
{{{{ scratchpad }}}}
</scratchpad>
{{% endif %}}

{{% if prev_reflections %}}
Previous reflections:
<prev_reflections>
{{{{ prev_reflections }}}}
</prev_reflections>
{{% endif %}}

{{% if interp_prompt %}}
<interp_prompt>
{{{{ interp_prompt }}}}
</interp_prompt>
{{% endif %}}
{agent_prompt.strip()}
""".strip()
    

class PentestBot(LMP):
    """Languageâ€‘model program that drives each reasoning step."""

    def __init__(self, agent_prompt: str):
        super().__init__()
        self.prompt = build_pentest_prompt(agent_prompt)
        self.response_format = AgentStep

# NOTE: this is stand-alone pentesting agent
# the code wrapping this is the lab setup code
class PentestSession:
    """Runs the LLM â†” interpreter feedback loop until success or exhaustion."""

    def __init__(
        self,
        *,
        problem_stmt: str,
        agent_prompt: str,
        model: LLMHub,
        max_steps: int = 12,
        success_condition: str | None = None,
        eval_fn: Optional[Callable[..., bool]] = None,
        tools: Optional[Dict[str, Dict[str, Any]]] = None,
        interpreter: Optional["PythonInterpreter"] = None,
        ) -> None:
        self.problem_stmt = problem_stmt
        self.max_steps = max_steps
        self.success_condition = success_condition or ""
        self._is_success = eval_fn or (lambda _answer: False)
        self._tools = tools or {}

        # TODO: move this code into interpreter
        # Build globals dict for interpreter
        shared_globals: Dict[str, Any] = {"TOOLS": self._tools}
        for _name, _cfg in self._tools.items():
            shared_globals[_name] = _cfg.get("tool")

        # Create lightweight in-memory module "tools" so that agent scripts can
        # use "import tools" or "from tools import browser_check_xss".
        if self._tools:
            import types, sys

            tools_mod = types.ModuleType("tools")
            for _name, _cfg in self._tools.items():
                func_obj = _cfg.get("tool")
                setattr(tools_mod, _name, func_obj)

                # Expose each tool as its own top-level module to support
                # import styles like `from browser_check_xss import browser_check_xss`.
                single_mod = types.ModuleType(_name)
                single_mod.__dict__[_name] = func_obj
                sys.modules[_name] = single_mod

            sys.modules["tools"] = tools_mod

        self._agent = PentestBot(agent_prompt=agent_prompt)
        # If an interpreter instance was provided, reuse it and update its globals with the
        # dynamically constructed shared_globals.  Otherwise build a fresh interpreter now.
        if interpreter is not None:
            # Update the caller-supplied interpreter so that any tool globals constructed above
            # are available when the agent-generated code executes.
            try:
                interpreter._globals.update(shared_globals)  # type: ignore[attr-defined]
            except AttributeError:
                # Fallback â€“ if the interpreter implementation does not expose _globals,
                # simply ignore and rely on the caller to have injected the necessary names.
                pass
            self._interp = interpreter
        else:
            # Default behaviour â€“ construct a new sandbox with the prepared globals.
            self._interp = PythonInterpreter(shared_globals=shared_globals)

        self._interp_prompt = self._interp.get_prompt()
        self._scratchpad = ""

        # models
        self._model = model
        self._agent_model = model.get("agent")

        # Sessionâ€‘level state
        self._agent_results: List[AgentResult] = []
        self._success: bool = False
        self._steps: int = 0

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def run(self) -> None:  # noqa: D401 â€“Â imperative mood
        """Drive the conversation loop."""

        prev_turn: Optional[SimpleNamespace] = None
        prev_reflections = ""

        for step in range(1, self.max_steps + 1):
            self._log("========== STEP %d ==========" % step)
            prompt_args = {
                "problem_stmt": self.problem_stmt,
                "prev_turn": prev_turn,
                "scratchpad": self._scratchpad.strip(),
                "prev_reflections": prev_reflections,
                "interp_prompt": self._interp_prompt
            }
            if step == 1:
                self._log("Interpreter Prompt: \n%s" % self._interp_prompt)
                self._debug_prompt(prompt_args)
            if step % 4 == 0:
                self._log("Prev Reflections: \n%s" % prev_reflections)

            # ----------------------------------------------------------------
            # LLM invocation
            # ----------------------------------------------------------------
            agent_step: AgentStep = self._agent.invoke(
                model=self._agent_model,
                prompt_args=prompt_args,
            )

            self._log("Reflection:\n%s" % agent_step.reflection.strip())
            full_log.info("Generated script:\n%s" % agent_step.script.rstrip())
            if self._scratchpad:
                self._log("Scratchpad:\n%s" % self._scratchpad.strip())

            # ----------------------------------------------------------------
            # Script execution
            # ----------------------------------------------------------------
            execution_result = self._interp.run(agent_step.script)
            self._log_execution_output(execution_result)

            self._agent_results.append(AgentResult(
                step_num=step,
                reflection=agent_step.reflection,
                script=agent_step.script,
                execution_output=execution_result,
            ))

            # Persist scratchpad additions for future turns
            if agent_step.add_to_scratchpad:
                self._add_to_scratchpad(agent_step.add_to_scratchpad, step)

            # Prepare context for the next iteration
            prev_turn = SimpleNamespace(
                reflection=agent_step.reflection,
                script=agent_step.script,
                output=execution_result,
            )
            prev_reflections += f"[{step}] {agent_step.reflection.strip()}\n"
            self._steps += 1
        else:
            self._log("ðŸ’€ Maximum steps (%d) reached without confirmed exploit." % self.max_steps)

    def result(self) -> Tuple[bool, int, int, str]:  # noqa: D401 â€“Â imperative mood
        """Return ``(success, steps_taken, max_steps, model_name)`` for the session."""

        return (
            self._success, 
            self._steps, 
            self.max_steps, 
            self._agent_model.model_name
        )

    def steps(self) -> List[AgentResult]:
        return self._agent_results

    def _log(self, msg: str) -> None:
        agent_log.info(msg)
        full_log.info(msg)

    def _debug_prompt(self, prompt_args: Dict[str, object]) -> None:
        prompt_preview = self._agent._prepare_prompt(templates={}, **prompt_args)
        agent_log.info(f"Prompt:\n{prompt_preview}")

    def _log_execution_output(self, output: str) -> None:
        full_log.info("Execution output:\n%s", output)
        agent_log.info("Execution output (truncated to %d chars):\n%s", MAX_OUTPUT_LOG_LEN, output[:MAX_OUTPUT_LOG_LEN])

    def _add_to_scratchpad(self, text: str, turn: int) -> None:
        self._scratchpad += f"[Turn {turn}] {text.strip()}\n"

class LaunchPentestBots(ABC):
    """
    â€¢ Creates a shared parent log dir
    â€¢ Spawns a ThreadPoolExecutor
    â€¢ Runs one agent per lab in parallel
    â€¢ Collects & prints results
        """

    def __init__(
        self,
        *,
        lab_urls: Sequence[LabInfo],
        model: LLMHub,
        agent_funcs: Dict[str, str],
        agent_prompt: str,
        max_steps: int = 12,
        include_description: bool = False,
        log_subfolder: str = "pentest_bot",
        is_eval: bool = False,
        comment: str | None = None,
        interp_cls: Type[PythonInterpreter] = PythonInterpreter,
        critic: bool = False,
        tools: Dict = {},
    ):
        self.lab_urls = lab_urls
        self.model_router = model
        self.agent_prompt = agent_prompt
        self.max_steps = max_steps
        self.include_description = include_description
        self.comment = comment
        self.interp_cls = interp_cls
        self.tools = tools
        self.critic = critic
        self.agent_funcs = agent_funcs

        # shared parent directory for **all** logs in this run
        parent_dir = create_log_dir_or_noop(subfolder=log_subfolder)
        self.parent_dir = run_id_dir(parent_dir)
        self.parent_dir.mkdir(parents=True, exist_ok=True)

        self._executor: concurrent.futures.ThreadPoolExecutor | None = None
        self.run_id = self._create_run_record(parent_dir, is_eval)

        # Ensure the results database is ready before any threads start.
        init_db()

    # ------------------------------------------------------------------ #
    # Database helpers                                                   #
    # ------------------------------------------------------------------ #
    def _create_run_record(self, parent_dir: Path, is_eval: bool) -> int:
        db = SessionLocal()
        try:
            run_rec = RunORM(
                comment=self.comment,
                parent_logdir=str(parent_dir),
                is_eval=is_eval,
            )
            db.add(run_rec)
            db.commit()
            db.refresh(run_rec)
            self.run_id = run_rec.id
            return self.run_id
        finally:
            db.close()

    def _delete_runs(self, force: bool = False) -> None:
        """Delete runs"""
        if force:
            db = SessionLocal()
            try:
                # Check if the run has any results before deleting
                has_results = db.query(PentestResultORM).filter(PentestResultORM.run_id == self.run_id).first() is not None
                
                if force or not has_results:
                    db.query(RunORM).filter(RunORM.id == self.run_id).delete()
                    db.commit()
            finally:
                db.close()

    def _create_results_record(
        self,
        eval_name: str,
        success: bool,
        steps: int,
        max_steps: int,
        model_name: str,
        model_costs: float,
        log_filepath: str,
    ) -> int | None:
        """Insert a single row into *pentest_results*.

        The function is deliberately minimal â€“ it just opens a new session,
        writes the record, and closes the session.  Error handling is kept
        intentionally broad because failed persistence must **never** crash
        the running agents.
        
        Returns:
            The ID of the newly created PentestResult record, or None if creation failed.
        """
        try:
            db = SessionLocal()
            pentest_result = PentestResultORM(
                run_id=cast(int, self.run_id),
                eval_name=eval_name,
                success=success,
                steps=steps,
                max_steps=max_steps,
                model_name=model_name,
                model_costs=model_costs,
                log_filepath=log_filepath,
            )
            db.add(pentest_result)
            db.commit()
            db.refresh(pentest_result)
            return pentest_result.id
        except Exception:  # noqa: BLE001 â€“ log & swallow
            logging.getLogger(LOGGER_NAME).exception("Failed to persist results for %s", eval_name)
        finally:
            try:
                db.close()
            except Exception:
                # ignore double-close or session errors
                pass

    def _create_agent_steps_record(self, agent_steps: List[AgentResult], step_types: List[LLMStep] | None, result_id: int) -> None:
        """Insert a single row into *agent_steps*."""
        try:
            db = SessionLocal()
            if not step_types:
                step_types = [None] * len(agent_steps)

            for i, (step, step_type) in enumerate(zip(agent_steps, step_types), start=1):
                db.add(AgentStepORM(
                    pentest_result_id=result_id,
                    step_number=i,
                    reflection=step.reflection,
                    script=step.script,
                    add_to_scratchpad=step.add_to_scratchpad,
                    step_type=step_type.type.value if step_type else None,
                    step_descr=step_type.summary if step_type else None,
                    execution_output=step.execution_output,
                ))
            db.commit()
        except Exception:  # noqa: BLE001 â€“ log & swallow
            agent_log.exception("Failed to persist agent steps for %s", self.run_id)

    def _install_sigint_handler(self) -> None:
        def _sigint(signum, frame):
            print("\n[!] SIGINT received â€“ terminating thread pool â€¦", file=sys.stderr)
            if self._executor:
                self._executor.shutdown(wait=False, cancel_futures=True)

            # delete all runs if sigint forces us
            self._delete_runs(force=True)
            logging.shutdown()
            os._exit(1)

        signal.signal(signal.SIGINT, _sigint)

    @abstractmethod
    def _prepare_lab(self, url: str,  lab_info: LabInfo) -> Tuple[str, Callable[[], bool], Dict]:
        """
        Returns:
            problem_statement: str
            eval_fn: Callable[[], bool]
            tools: Dict
        """
        raise NotImplementedError("Subclass must implement this method")
    
    def start_pentest_session(self, 
                              model: LLMHub,
                              problem_statement: str, 
                              eval_fn: Callable[[], bool], 
                              agent_tools: Dict, 
                              agent_prompt_full: str,
                              thread_id: int):
        """Starts a pentest session for a single lab"""
        interpreter = self.interp_cls(shared_globals={})
        session = PentestSession(
            problem_stmt=problem_statement,
            model=model,
            agent_prompt=agent_prompt_full,
            max_steps=self.max_steps,
            eval_fn=eval_fn,
            tools=agent_tools,
            interpreter=interpreter,
        )
        session.run()
        return session

    def start_agent(
        self,
        lab_info: LabInfo,
        thread_id: int
    ) -> Tuple[LabInfo, Optional[Path]]:
        """
        Launches a single PentestSession for one lab.
        Signature identical to the old free function (model & dirs taken
        from self).
        """
        try:
            set_ctxt_id(str(lab_info.lab_ind)) # per-lab context ID

            # pass a separate instance of model to each agent so we can track costs separately
            model_router = LLMHub(self.agent_funcs)

            _, log_filepath = setup_agent_logger(
                "agent_log",
                parent_dir=self.parent_dir,
            )
            agent_log = logging.getLogger("agentlog")
            agent_log.info("agent started for %s", lab_info.name)

            url = lab_info.url
            problem_statement, eval_fn, tools_dict = self._prepare_lab(url, lab_info)
            agent_tools = {**self.tools, **tools_dict}

            tools_desc_block = ""
            if agent_tools:
                tools_json = json.dumps(
                    {name: t["description"] for name, t in agent_tools.items()}, indent=2
                )
                tools_desc_block = (
                    "Here are some tools that you can call in your script:\n\n" + tools_json
                )

            agent_prompt_full = self.agent_prompt + tools_desc_block

            # DESIGN NOTE:
            # we depend on _prepare_lab to fully setup an interpreter instance, not best design  
            session = self.start_pentest_session(model_router, problem_statement, eval_fn, agent_tools, agent_prompt_full, thread_id)
            success, steps, max_steps, model_name = session.result()
            agent_steps = session.steps()

            if self.critic:
                model = self.model_router.get("step_labeler")
                step_labeler = LabelAgentSteps()
                step_types= step_labeler.invoke(model, prompt_args={"results": agent_steps})
            else:
                step_types = None

            # TODO: fix the cost here to accomodate the critic
            result_id = self._create_results_record(
                lab_info.name,
                success,
                steps,
                max_steps,
                model_name,
                # we sum costs once both the critic and the agent are done
                sum([v for _, v in model_router.get_costs().items()]),
                str(log_filepath)
            )
            self._create_agent_steps_record(agent_steps, step_types.steps if step_types else None, result_id)
            return lab_info, log_filepath

        except Exception:
            logging.getLogger("agentlog").exception("Uncaught error in %s", lab_info.name)
            return lab_info, None
        
    # TODO: this fails for some reason
    def _consolidate_logs(self, lab_paths: Dict[int, List[Path]]) -> None:
        """
        For each lab index (int key), concatenate all of its logs together
        and write to a separate file in the parent directory.

        Result files are written to <parent_dir>/concat_agent_logs_lab_{lab_ind}.log
        """
        for lab_ind, paths in lab_paths.items():
            output = self.parent_dir / f"concat_agent_logs_lab_{lab_ind}.log"

            with output.open("w", encoding="utf-8") as out_f:
                for log_path in sorted(paths):
                    header = f"\n### {log_path.parent.name}/{log_path.name} ###\n"
                    out_f.write(header)
                    try:
                        shutil.copyfileobj(log_path.open("r", encoding="utf-8"), out_f)
                    except Exception:
                        logging.getLogger(LOGGER_NAME).warning("Failed to copy log file %s", log_path)
                
                # write critique instructions
                out_f.write(f"\n### Critique Instructions ###\n")
                out_f.write(AGENT_CRITIQUE_PROMPT)

            print(f"[+] Consolidated logs for lab {lab_ind} written to {output}")

    def start_labs(self) -> None:
        self._install_sigint_handler()
        self._executor = concurrent.futures.ThreadPoolExecutor()
        
        lab_paths = defaultdict(list)
        futures = [
            self._executor.submit(self.start_agent, lab, thread_id) 
            for thread_id, lab 
            in enumerate(self.lab_urls, start=1)
        ]

        try:
            while futures:
                done, futures = concurrent.futures.wait(
                    futures, timeout=0.25, return_when=concurrent.futures.FIRST_COMPLETED
                )

                # â”€â”€ NEW: collect results as futures complete â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                for fut in done:
                    try:
                        lab_info, log_path = fut.result()
                        if log_path is not None:
                            lab_paths[lab_info.lab_ind].append(log_path)
                    except Exception:
                        logging.getLogger("agentlog").exception("Future processing failed")
        except KeyboardInterrupt:
            sys.exit()
        finally:
            self._executor.shutdown(wait=False)
            # Flush & close all logging handlers to release file locks (especially on Windows).
            logging.shutdown()
            self._consolidate_logs(lab_paths)