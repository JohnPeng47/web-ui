import re, urllib.parse, textwrap, itertools
import requests
from collections import defaultdict

base = "https://app.aikido.dev"
req = requests

print("Fetching root page …")
root_resp = req.get(base + "/")
root_html = root_resp.text
print(f"root status {root_resp.status_code}, len={len(root_html)}")

# ---------------------------------------------------------------------------
# 1.  Find bundle URLs exactly as before
# ---------------------------------------------------------------------------
srcs = re.findall(r"<script[^>]+src=\"([^\"]+\.js)\"", root_html)[:10]
print("Found JS bundle sources:")
for s in srcs:
    print("  ", s)

# ---------------------------------------------------------------------------
# 2.  Scan JS for /api/… *and* capture a context window around each match
# ---------------------------------------------------------------------------
endpoint_re = re.compile(r"(/api/[A-Za-z0-9_./-]+)")
endpoints: set[str] = set()
contexts: dict[str, list[str]] = defaultdict(list)
context_len = 300


for src in srcs:
    js_url = urllib.parse.urljoin(base, src)
    try:
        r = req.get(js_url, timeout=15)
        if r.status_code != 200 or "javascript" not in r.headers.get("content-type", ""):
            print(f"Skip {js_url} status {r.status_code}")
            continue
        body = r.text
        for m in endpoint_re.finditer(body):
            start, end = m.span(1)
            path = m.group(1).rstrip("\"') }")
            endpoints.add(path)

            # 120 chars before / after to show fetch/axios call, params, etc.
            snippet = body[max(0, start - context_len) : min(len(body), end + context_len)]
            snippet = " ".join(snippet.split())           # single-line
            snippet = textwrap.shorten(snippet, width=context_len, placeholder="…")
            if len(contexts[path]) < 2:                   # keep it short
                contexts[path].append(snippet)
        print(f"Scanned {js_url} (len {len(body)}) – "
              f"found {len(endpoints)} cumulative api strings")
    except Exception as e:
        print(f"Error fetching {js_url}: {e}")

# ---------------------------------------------------------------------------
# 3.  Pretty-print endpoints plus one example context
# ---------------------------------------------------------------------------
print("\nUnique candidate API endpoints and sample call sites:")
for ep in sorted(endpoints):
    sample = contexts[ep][0] if contexts[ep] else ""
    # quick heuristic to guess verb
    verb = "UNKNOWN"
    lower = sample.lower()
    for v in ["get", "post", "put", "delete", "patch"]:
        if f"axios.{v}" in lower or f"method:\"{v}\"" in lower or f"method: \"{v}\"" in lower:
            verb = v.upper()
            break
        if f"fetch(" in lower and f"\"{v}\"" in lower:
            verb = v.upper()
    print(f"  {verb:7} {ep}\n      → {sample}")

# ---------------------------------------------------------------------------
# 4.  Probe up to ten endpoints with a raw GET (no auth) as before
# ---------------------------------------------------------------------------
# print("\nProbing up to 10 endpoints with GET (no auth)…")
# for ep in itertools.islice(sorted(endpoints), 10):
#     try:
#         r = req.get(base + ep, timeout=15)
#         ctype = r.headers.get("content-type", "")
#         print(f"{ep} -> {r.status_code} {ctype} len={len(r.text)}")
#     except Exception as e:
#         print(f"{ep} -> EXC {e}")
