from __future__ import annotations

"""Refactored PentestBot driver.

The original script mixed prompt‑building, agent orchestration, and sandbox
execution logic in a single, monolithic file.  This rewrite introduces:

* Clear module‑level separation of concerns
* Consistent logging and typing throughout
* Consolidated configuration constants
* Defensive error handling with early exits
* PEP‑8 compliant spacing and double quotes
* Explicit public API surface (`run()` and `result()`)

The code remains a single file for ease of distribution, but the internal
layout mirrors how you would structure the project if/when you split it into
multiple modules (``prompts.py``, ``interpreter.py``, etc.).
"""

import concurrent.futures
import shutil
import json
import logging
import os
import signal
import sys
import traceback
from contextlib import redirect_stderr, redirect_stdout
from dataclasses import dataclass
from io import StringIO
from pathlib import Path
from types import SimpleNamespace
from typing import Any, Callable, Dict, List, Optional, Tuple, TypeVar, cast

from pydantic import BaseModel

from src.llm_models import LLMHub
from src.llm_provider import LMP
from src.utils.context import set_ctxt_id

from scripts.portswigger.data import PORT_SWIGGER_LABS, PROMPT_TEMPLATES, XSS_APPRENTICE_LABS  # type: ignore – project‑specific
from pentest_bot.logger import get_agent_loggers, setup_agent_logger, create_log_dir_or_noop, run_id_dir

from .xss_client import register_xss_payload
from .tools import TOOLS as TOOLS_TEMPLATE, create_browser_check_xss
from .prompts import AGENT_PROMPT_XSS as AGENT_PROMPT, AGENT_CRITIQUE_PROMPT

from pentest_bot.db import init_db, SessionLocal, PentestResult, Run

T = TypeVar("T")
LOGGER_NAME = "agentlog"
MAX_OUTPUT_LOG_LEN = 8192  # characters

agent_log, full_log = get_agent_loggers()

LLM_FUNCS = {
    "agent": "o3",
}

class AgentOutput(BaseModel):
    """LLM response schema expected by :pymeth:`PentestBot.invoke`."""

    reflection: str
    script: str
    answer: Optional[Dict] = None
    add_to_scratchpad: Optional[str] = None

    def is_complete(self) -> bool:  # noqa: D401 – imperative mood
        """Return *True* iff the agent signalled task completion."""

        return "TASK_COMPLETE" in self.script or "TASK_COMPLETE" in self.reflection

def build_pentest_prompt(task_prompt: str) -> str:
    """Inject *task_prompt* into the shared Jinja template returned to the LLM."""

    return f"""
{{% if problem_stmt %}}
<problem_stmt>
Here is the CTF problem statement:
{{{{ problem_stmt }}}}
</problem_stmt>
{{% endif %}}

{{% if prev_turn %}}
<prev_reflection>
Here is the reflection from the previous action:
{{{{ prev_turn.reflection }}}}
</prev_reflection>

<prev_script>
Here is the generated script:
{{{{ prev_turn.script }}}}
</prev_script>

<prev_output>
Here is the output from the previous action:
{{{{ prev_turn.output }}}}
</prev_output>
{{% endif %}}

{{% if scratchpad %}}
Scratchpad (context from previous turns):
<scratchpad>
{{{{ scratchpad }}}}
</scratchpad>
{{% endif %}}

{task_prompt.strip()}
""".strip()
    
class PythonInterpreter:
    """Minimal, state‑preserving Python execution sandbox."""

    def __init__(self, shared_globals: Optional[Dict[str, object]] = None) -> None:
        self._globals: Dict[str, object] = shared_globals or {}

    # ---------------------------------------------------------------------
    # Public helpers
    # ---------------------------------------------------------------------

    def run(self, code: str) -> str:
        """Execute *code* and return ``stdout`` + ``stderr``.

        Fatal exceptions (*not* derived from plain :class:`Exception`) are
        re-raised so they propagate to the caller – this lets Ctrl-C
        (:class:`KeyboardInterrupt`) or :pyfunc:`sys.exit` terminate the whole
        program promptly while still capturing any partial output for logging.
        """

        stdout_buf = StringIO()
        stderr_buf = StringIO()

        try:
            with redirect_stdout(stdout_buf), redirect_stderr(stderr_buf):
                exec(code, self._globals, {})
        except KeyboardInterrupt:  # ⇢ Ctrl-C
            print("KeyboardInterrupt")
            raise
        except SystemExit:  # ⇢ sys.exit()
            raise
        except Exception:  # normal runtime error – capture traceback, continue
            traceback.print_exc(file=stderr_buf)
        except BaseException:  # any other fatal (asyncio.CancelledError, etc.)
            traceback.print_exc(file=stderr_buf)
            raise

        stdout_text = stdout_buf.getvalue()
        stderr_text = stderr_buf.getvalue()
        return f"{stdout_text}{os.linesep if stderr_text else ''}{stderr_text}"

class PentestBot(LMP):
    """Language‑model program that drives each reasoning step."""

    def __init__(self, agent_prompt: str):
        super().__init__()
        self.prompt = build_pentest_prompt(agent_prompt)
        self.response_format = AgentOutput

# NOTE: this is stand-alone pentesting agent
# the code wrapping this is the lab setup code
class PentestSession:
    """Runs the LLM ↔ interpreter feedback loop until success or exhaustion."""

    def __init__(
        self,
        *,
        problem_stmt: str,
        agent_prompt: str,
        model: LLMHub,
        max_steps: int = 12,
        success_condition: str | None = None,
        eval_fn: Optional[Callable[..., bool]] = None,
        tools: Optional[Dict[str, Dict[str, Any]]] = None,
    ) -> None:
        self.problem_stmt = problem_stmt
        self.max_steps = max_steps
        self.success_condition = success_condition or ""
        self._is_success = eval_fn or (lambda _answer: False)
        self._tools = tools or {}

        # Build globals dict for interpreter
        shared_globals: Dict[str, Any] = {"TOOLS": self._tools}
        for _name, _cfg in self._tools.items():
            shared_globals[_name] = _cfg.get("tool")

        # Create lightweight in-memory module "tools" so that agent scripts can
        # use "import tools" or "from tools import browser_check_xss".
        if self._tools:
            import types, sys

            tools_mod = types.ModuleType("tools")
            for _name, _cfg in self._tools.items():
                func_obj = _cfg.get("tool")
                setattr(tools_mod, _name, func_obj)

                # Expose each tool as its own top-level module to support
                # import styles like `from browser_check_xss import browser_check_xss`.
                single_mod = types.ModuleType(_name)
                single_mod.__dict__[_name] = func_obj
                sys.modules[_name] = single_mod

            sys.modules["tools"] = tools_mod

        self._agent = PentestBot(agent_prompt=agent_prompt)
        self._interp = PythonInterpreter(shared_globals=shared_globals)
        self._scratchpad = ""

        # models
        self._model = model
        self._agent_model = model.get("agent")

        # Session‑level state
        self._success: bool = False
        self._steps: int = 0

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def run(self) -> None:  # noqa: D401 – imperative mood
        """Drive the conversation loop."""

        prev_turn: Optional[SimpleNamespace] = None

        for step in range(1, self.max_steps + 1):
            agent_log.info("========== STEP %d ==========" % step)
            prompt_args = {
                "problem_stmt": self.problem_stmt,
                "prev_turn": prev_turn,
                "scratchpad": self._scratchpad.strip(),
            }
            if step == 1:
                self._debug_prompt(prompt_args)

            # ----------------------------------------------------------------
            # LLM invocation
            # ----------------------------------------------------------------
            agent_out: AgentOutput = self._agent.invoke(
                model=self._agent_model,
                prompt_args=prompt_args,
            )

            # ----------------------------------------------------------------
            # Success evaluation (answers + external callback)
            # ----------------------------------------------------------------

            # 1) If the agent provided an explicit answer, prefer that branch.
            if agent_out.answer is not None:
                self._success = bool(self._is_success(agent_out.answer))
                if self._success:
                    agent_log.info("Agent successfully completed task with answer.")
                    break
                raise ValueError(f"Wrong answer! : {agent_out.answer}")

            # 2) Otherwise fall back to a side-effect based success predicate
            #    (e.g. an external polling callback for XSS payload execution).
            self._pending_external_success_check = True  # flag to evaluate later

            agent_log.info(f"Reflection:\n{agent_out.reflection.strip()}")
            agent_log.info(f"Generated script:\n{agent_out.script.rstrip()}")

            # ----------------------------------------------------------------
            # Script execution
            # ----------------------------------------------------------------
            execution_result = self._interp.run(agent_out.script)
            self._log_execution_output(execution_result)

            # After executing the agent script, run the external success
            # predicate (if any) **now** because the exploit's side-effects
            # may only manifest after the code above runs.
            if getattr(self, "_pending_external_success_check", False):
                try:
                    if self._is_success():  # type: ignore[arg-type]
                        self._success = True
                        agent_log.info("External eval_fn confirmed success.")
                        break
                except TypeError:
                    # Signature mismatch; ignore.
                    pass

            # Persist scratchpad additions for future turns
            if agent_out.add_to_scratchpad:
                self._add_to_scratchpad(agent_out.add_to_scratchpad, step)

            # Prepare context for the next iteration
            prev_turn = SimpleNamespace(
                reflection=agent_out.reflection,
                script=agent_out.script,
                output=execution_result,
            )
            self._steps += 1
        else:
            agent_log.warning(
                "💀 Maximum steps (%d) reached without confirmed exploit.", self.max_steps,
            )

    def result(self) -> Tuple[bool, int, int, str, float]:  # noqa: D401 – imperative mood
        """Return ``(success, steps_taken, max_steps, model_name, model_costs)`` for the session."""

        return (
            self._success, 
            self._steps, 
            self.max_steps, 
            self._agent_model.model_name,
            sum([v for _, v in self._model.get_costs().items()])
        )

    def _log(self, msg: str) -> None:
        agent_log.info(msg)
        full_log.info(msg)

    def _debug_prompt(self, prompt_args: Dict[str, object]) -> None:
        prompt_preview = self._agent._prepare_prompt(templates={}, **prompt_args)
        agent_log.info(f"Prompt:\n{prompt_preview}")

    def _log_execution_output(self, output: str) -> None:
        full_log.info("Execution output:\n%s", output)
        agent_log.info("Execution output (truncated to %d chars):\n%s", MAX_OUTPUT_LOG_LEN, output[:MAX_OUTPUT_LOG_LEN])

    def _add_to_scratchpad(self, text: str, turn: int) -> None:
        self._scratchpad += f"[Turn {turn}] {text.strip()}\n"


@dataclass
class AgentResult:
    eval_name: str
    success: bool
    steps: int
    max_steps: int

@dataclass
class LabInfo:
    vuln_class: str
    lab_ind: int

    @property
    def name(self) -> str:
        return f"{self.vuln_class}_{self.lab_ind}"

# TODO: modify the sigint handler to do run cleanup logic
class LaunchPentestBots:
    """
    • Creates a shared parent log dir
    • Spawns a ThreadPoolExecutor
    • Runs one agent per lab in parallel
    • Collects & prints results
    """

    def __init__(
        self,
        *,
        lab_urls: List[Tuple[str, str, int]],
        model: LLMHub,
        agent_prompt: str,
        max_steps: int = 12,
        include_description: bool = False,
        log_subfolder: str = "pentest_bot",
        eval: bool = False,
        comment: str | None = None,
    ):
        self.lab_urls = lab_urls
        self.model = model
        self.agent_prompt = agent_prompt
        self.max_steps = max_steps
        self.include_description = include_description
        self.results: Dict[str, AgentResult] = {}
        self.comment = comment

        # shared parent directory for **all** logs in this run
        parent_dir = create_log_dir_or_noop(subfolder=log_subfolder, no_date=True)
        self.parent_dir = run_id_dir(parent_dir)
        self.parent_dir.mkdir(parents=True, exist_ok=True)

        self._executor: concurrent.futures.ThreadPoolExecutor | None = None

        self.eval = eval
        self.run_id = self._create_run_record(parent_dir)

        # Ensure the results database is ready before any threads start.
        init_db()

    # ------------------------------------------------------------------ #
    # Database helpers                                                   #
    # ------------------------------------------------------------------ #
    def _create_run_record(self, parent_dir: Path) -> int:
        if self.eval:
            db = SessionLocal()
            try:
                run_rec = Run(
                    comment=self.comment,
                    parent_logdir=str(parent_dir),
                )
                db.add(run_rec)
                db.commit()
                db.refresh(run_rec)
                self.run_id = run_rec.id
                return self.run_id
            finally:
                db.close()

    def _delete_runs(self, force: bool = False) -> None:
        """Delete runs"""
        if self.eval:
            if force or not self.results:
                db = SessionLocal()
                try:
                    # Check if the run has any results before deleting
                    has_results = db.query(PentestResult).filter(PentestResult.run_id == self.run_id).first() is not None
                    
                    if force or not has_results:
                        db.query(Run).filter(Run.id == self.run_id).delete()
                        db.commit()
                finally:
                    db.close()

    def _create_results_record(
        self,
        eval_name: str,
        success: bool,
        steps: int,
        max_steps: int,
        model_name: str,
        model_costs: float,
        log_filepath: str,
    ) -> None:
        """Insert a single row into *pentest_results*.

        The function is deliberately minimal – it just opens a new session,
        writes the record, and closes the session.  Error handling is kept
        intentionally broad because failed persistence must **never** crash
        the running agents.
        """
        try:
            db = SessionLocal()
            db.add(
                PentestResult(
                    run_id=cast(int, self.run_id),
                    eval_name=eval_name,
                    success=success,
                    steps=steps,
                    max_steps=max_steps,
                    model_name=model_name,
                    model_costs=model_costs,
                    log_filepath=log_filepath,
                )
            )
            db.commit()
        except Exception:  # noqa: BLE001 – log & swallow
            logging.getLogger(LOGGER_NAME).exception("Failed to persist results for %s", eval_name)
        finally:
            try:
                db.close()
            except Exception:
                # ignore double-close or session errors
                pass

    def _install_sigint_handler(self) -> None:
        def _sigint(signum, frame):
            print("\n[!] SIGINT received – terminating thread pool …", file=sys.stderr)
            if self._executor:
                self._executor.shutdown(wait=False, cancel_futures=True)

            # delete all runs if sigint forces us
            self._delete_runs(force=True)
            logging.shutdown()
            os._exit(1)

        signal.signal(signal.SIGINT, _sigint)

    def _run_agent(self, lab_info: Tuple[str, str, int]) -> None:
        info = LabInfo(vuln_class=lab_info[1], lab_ind=lab_info[2] + 1) # NOTE: fix this off by one error
        url = lab_info[0]
        
        set_ctxt_id(str(lab_info[2]))                # per-lab context ID
        success, steps, max_steps, model_name, model_costs = self.start_agent(url, info)
        # self.results[eval_name] = AgentResult(eval_name, success, steps, max_steps)

    def start_agent(
        self,
        url: str,
        lab_info: LabInfo,
    ) -> Tuple[bool, int, int, str, float]:
        """
        Launches a single PentestSession for one lab.
        Signature identical to the old free function (model & dirs taken
        from self).
        """
        try:
            vuln_class = lab_info.vuln_class
            lab_ind = lab_info.lab_ind

            labs = PORT_SWIGGER_LABS[vuln_class]
            key = "with_description" if self.include_description else "without_description"
            problem_statement = PROMPT_TEMPLATES[vuln_class][key]

            # thread/agent-specific loggers
            _, log_filepath = setup_agent_logger(
                "agent_log",
                parent_dir=self.parent_dir,
                no_date=True
            )
            agent_log = logging.getLogger("agentlog")
            agent_log.info("agent started for %s", lab_info.name)

            # ---------------------------------------------------------- #
            # optional XSS tooling & prompt injection                    #
            # ---------------------------------------------------------- #
            tools: Dict[str, Dict[str, Any]] = {}
            eval_fn_override = None

            if vuln_class == "cross_site_scripting":
                client_cb, target_url = register_xss_payload()
                tool_fn = create_browser_check_xss(client_cb)

                tools = {
                    "browser_check_xss": {
                        "description": TOOLS_TEMPLATE["browser_check_xss"]["description"],
                        "tool": tool_fn,
                    }
                }

                if self.include_description:
                    problem_statement = problem_statement.format(
                        url=url,
                        description=labs[lab_ind].get("description", ""),
                        target_url=target_url,
                    )
                else:
                    problem_statement = problem_statement.format(url=url, target_url=target_url)

                eval_fn_override = client_cb
            else:
                eval_fn_override = labs[lab_ind].get("eval_fn")

            # ---------------------------------------------------------- #
            # final agent prompt                                         #
            # ---------------------------------------------------------- #
            tools_desc_block = ""
            if tools:
                tools_json = json.dumps(
                    {name: t["description"] for name, t in tools.items()}, indent=2
                )
                tools_desc_block = (
                    "Here are some tools that you can call in your script:\n\n" + tools_json
                )

            agent_prompt_full = self.agent_prompt + tools_desc_block

            session = PentestSession(
                problem_stmt=problem_statement,
                model=self.model,
                agent_prompt=agent_prompt_full,
                max_steps=self.max_steps,
                eval_fn=eval_fn_override,
                tools=tools
            )
            session.run()
            success, steps, max_steps, model_name, model_costs = session.result()

            # Persist the outcome of this run if eval mode
            if self.eval:
                self._create_results_record(
                    lab_info.name,
                    success,
                    steps,
                    max_steps,
                    model_name,
                    model_costs,
                    str(log_filepath)
                )
            return session.result()

        except Exception:
            logging.getLogger("agentlog").exception("Uncaught error in %s", lab_info.name)
            return False, 0, self.max_steps
        
    def _consolidate_logs(self) -> None:
        """
        Concatenate every *.log* that lives directly inside each child
        run-directory (0/, 1/, 2/, …) **but skip** anything under
        `full_requests/`.

        Result is written to  <parent_dir>/concat_agent_logs.log
        """
        output = self.parent_dir / "concat_agent_logs.log"

        with output.open("w", encoding="utf-8") as out_f:
            for child in sorted(self.parent_dir.iterdir(), key=lambda p: p.name):
                if not child.is_dir():
                    continue

                # grab every *.log in that child folder's top level
                for log_path in child.glob("*.log"):
                    header = f"\n### {child.name}/{log_path.name} ###\n"
                    out_f.write(header)
                    shutil.copyfileobj(log_path.open("r", encoding="utf-8"), out_f)
                
            # write critique insructions
            out_f.write(f"\n### Critique Instructions ###\n")
            out_f.write(AGENT_CRITIQUE_PROMPT)

        print(f"[+] Consolidated logs written to {output}")

    def start_labs(self) -> None:
        self._install_sigint_handler()
        self._executor = concurrent.futures.ThreadPoolExecutor()

        futures = [self._executor.submit(self._run_agent, lab) for lab in self.lab_urls]

        try:
            while futures:
                done, futures = concurrent.futures.wait(
                    futures, timeout=0.25, return_when=concurrent.futures.FIRST_COMPLETED
                )
        except KeyboardInterrupt:
            sys.exit()
        finally:
            self._delete_runs()
            self._executor.shutdown(wait=False)
            self._consolidate_logs()

        # -------- summary ---------- #
        print("\nAgent Results:")
        print("-" * 50)
        for res in self.results.values():
            status = "✅ Success" if res.success else "❌ Failed"
            print(f"{res.eval_name}: {status} (Steps: {res.steps}/{res.max_steps})")

        print("MODEL COST:", self.model.get_costs())


# --------------------------------------------------------------------------- #
# __main__                                                                    #
# --------------------------------------------------------------------------- #

# TODOS:
# - display the matched vuln cases in cli_db
# - display the config variables
if __name__ == "__main__":
    import argparse
    
    # TODO: store these config variables
    # think 3 is the ideal number for analysis
    MAX_LABS = 3
    MAX_STEPS = 12
    # how to test for stable value of this?
    NUM_TRIALS = 1

    parser = argparse.ArgumentParser(description="Run PentestBot labs")
    parser.add_argument("--eval", action="store_true", help="Persist results to DB")
    parser.add_argument("--comment", type=str, help="Comment describing this run")
    args = parser.parse_args()

    if args.eval and not args.comment:
        raise ValueError("--comment is required when --eval is set")

    LAB_URLS = [
        ("https://0a3400e004224d3582de5bbb008e0034.web-security-academy.net/", "cross_site_scripting", 8),
        ("https://0a5e004503e5407480060d9b00cf0062.web-security-academy.net/", "cross_site_scripting", 5),
        ("https://0acd007b042798f68039e91f002f009a.web-security-academy.net/", "cross_site_scripting", 7),
        ("https://0a77002603512830817680fd00f900bd.web-security-academy.net/", "cross_site_scripting", 9),
    ][:MAX_LABS]

    bots = LaunchPentestBots(
        lab_urls=LAB_URLS,
        model=LLMHub(function_map=LLM_FUNCS),
        agent_prompt=AGENT_PROMPT,
        max_steps=MAX_STEPS,
        include_description=False,
        log_subfolder="xss_agent/o3",
        eval=args.eval,
        comment=args.comment,
    )
    bots.start_labs()
